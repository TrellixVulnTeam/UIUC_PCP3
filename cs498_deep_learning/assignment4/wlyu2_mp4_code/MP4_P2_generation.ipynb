{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate\n",
    "\n",
    "from gru.model import GRU\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "train len:  1003854\n",
      "test len:  111540\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l not be bruising to you,\n",
      "When he hath power to crush? Why, had your bodies\n",
      "No heart among you? or had you tongues to cry\n",
      "Against the rectorship of judgment?\n",
      "\n",
      "SICINIUS:\n",
      "Have you\n",
      "Ere now denied the aske\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.0001\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    \n",
    "    #init_hidden, zero_grad\n",
    "    #loop\n",
    "    #backward, step, outside loop\n",
    "    \n",
    "    #loop 0 to chunk_len\n",
    "    #   output, hidden = \n",
    "    \n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    \n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    #print(chunk_len, batch_size, hidden.size())\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, i], hidden)\n",
    "        \n",
    "    #need to expand the dimensions of x, where the dimension are (sequence length, batch size, hidden)\n",
    "        loss += criterion(output.view(batch_size,-1), target[:, i])\n",
    "    #loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss.data.item() / chunk_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 38s (50 1%) train loss: 4.3371, test_loss: 4.3270]\n",
      "WhXd=g2L9c;\n",
      "Y^O6==k$m:C0hoO2\\|p]{V,U8k+IMMFf2K\"on\"$0|a[%!}Wt`\\tTc\n",
      "g;`~Ggo^ICR]\"8\u000b",
      "B&\u000b",
      "^+S?JJGUC \n",
      "\n",
      "[1m 14s (100 2%) train loss: 3.6252, test_loss: 3.6470]\n",
      "Wh^$W?IncD=\"V\n",
      "eR g?\\,b\"iu/tWDsmZv[ Au pieInefe6gnw4RZ.rHTgbcy9iNbGv<s1[gtDze?Jcp lZinhoeczI^i\f",
      "rQ 'm8Ty \n",
      "\n",
      "[1m 50s (150 3%) train loss: 3.3033, test_loss: 3.3193]\n",
      "Wh.akfeT heman eh\n",
      " ]2Ke sf|]*k  owrer h     hb\"k '  a`L\n",
      "l tt oe i,dyd angltTmNrTta hae e t |oinnj I e, \n",
      "\n",
      "[2m 26s (200 4%) train loss: 3.1707, test_loss: 3.1764]\n",
      "WhZ\u000b",
      "5gCssv8d  addo ne erekonoYlltlo,y mi~,v\n",
      "deiIs te>\n",
      "r? o atO8un:b\n",
      "nit ce sr  f' rgh, c gn  at\n",
      "m fa R \n",
      "\n",
      "[3m 3s (250 5%) train loss: 3.0843, test_loss: 3.0840]\n",
      "WhV;lreu\n",
      " .aII y we Yanddm ttet i?| t rhse  nes othor r' cfd.h'rs triiintcisodryr he my\n",
      "Wlsu pokd he.  \n",
      "\n",
      "[3m 39s (300 6%) train loss: 2.9717, test_loss: 3.0197]\n",
      "WhS+jtZinT wot di w a,h,w\n",
      "\n",
      "\n",
      "rLc srkem haet t y mer Je\n",
      "' eol woc tanear Ihenhowhe le! the\n",
      "he let el tot \n",
      "\n",
      "[4m 15s (350 7%) train loss: 2.9093, test_loss: 2.9356]\n",
      "Wher heo aere w1ne go,.e+\n",
      "het p\n",
      "\n",
      "9`op ben wous. oorhaw nd e Inosin uoc th iy r\n",
      "s rofrns ou ilte' ie wd \n",
      "\n",
      "[4m 52s (400 8%) train loss: 2.8485, test_loss: 2.8553]\n",
      "Wh*tos tahemoutna Wung7t t hawgee the frutf llere yaitet ncogt rhde \n",
      "r ha\n",
      "Yery soyoong athr ae$ea Ian  \n",
      "\n",
      "[5m 28s (450 9%) train loss: 2.7823, test_loss: 2.8015]\n",
      "e nheii st fre e f sy b\f",
      "ais fate notsoe net- taint se hytl she \n",
      " got mosuy mve t \n",
      "\n",
      "[6m 4s (500 10%) train loss: 2.7161, test_loss: 2.7391]\n",
      "WhgLtoy Ihet at,\n",
      "Gou botnel end it filod nt wineeh wyrcorsr, uabmLed,eshi, donm yo theddo the  ceanoe  \n",
      "\n",
      "[6m 41s (550 11%) train loss: 2.6737, test_loss: 2.6906]\n",
      "Whidt stdg\n",
      "\n",
      "G\tt e I m, yre?\n",
      "\n",
      "Wfonthal tagod chalr ggman inf ne temecr cith no seores onov soe aa thein \n",
      "\n",
      "[7m 17s (600 12%) train loss: 2.6440, test_loss: 2.6360]\n",
      "Whare mhe hand bemile ail pthtian ceou Ire bes a kerre piiaw, ae atartirerd he wat,.\n",
      "SI heresdlst  af  \n",
      "\n",
      "[7m 53s (650 13%) train loss: 2.5810, test_loss: 2.5889]\n",
      "Whaste nmete sou wand tomero,eisoucal~, grer_oRel atha his tht macerom howm thowe ,he'r ser\n",
      "\n",
      "onint aos \n",
      "\n",
      "[8m 29s (700 14%) train loss: 2.5533, test_loss: 2.5445]\n",
      "Wher those\n",
      "her Iod res ato hel this titou\n",
      "\n",
      "ABteof wes  thelTI  this lr m\n",
      "Ron*y my.\n",
      "\n",
      "I4up and touuhe ip \n",
      "\n",
      "[9m 8s (750 15%) train loss: 2.5213, test_loss: 2.5264]\n",
      "Whip are toe, beaour at thi gmou bouss thard hor wors aancouse ib k fuve outn-,\n",
      "Th tolr grenor weanve  \n",
      "\n",
      "[9m 49s (800 16%) train loss: 2.4943, test_loss: 2.4928]\n",
      "Whadseres yout die shou Risde harsiuher hes pin ali thar brid the thor nore tost phe aat ares,\n",
      "Tr yon. \n",
      "\n",
      "[10m 32s (850 17%) train loss: 2.4608, test_loss: 2.4379]\n",
      "Whe ing mour poucathell ve helle io fare toud wery suk the ee that tour id, my, bon and hn myerse, nom \n",
      "\n",
      "[11m 18s (900 18%) train loss: 2.4349, test_loss: 2.4446]\n",
      "Who by thatterort\n",
      "\n",
      "He ouse ttend the qane an ou fid seal her wobe seehas! fear, se mare dy pith Oos so \n",
      "\n",
      "[12m 6s (950 19%) train loss: 2.4027, test_loss: 2.4023]\n",
      "Wheer seard stasher ther tom me undr therc in berawised malis herhetrenie heeds sao thes.\n",
      "\n",
      "\n",
      "SCLIOUS:\n",
      "O \n",
      "\n",
      "[12m 59s (1000 20%) train loss: 2.3994, test_loss: 2.3976]\n",
      "Wh phaant.\n",
      "Te alen once bongasi otr as ary ave ave lerates kes oerir he must ale pin wiw ead glis is t \n",
      "\n",
      "[13m 51s (1050 21%) train loss: 2.3764, test_loss: 2.3567]\n",
      "Whit sauriss ans ende 'ont ant I how he spindtle corthers sut\n",
      "%\n",
      "Thald, soy mash tho pan hof Conde on t \n",
      "\n",
      "[14m 42s (1100 22%) train loss: 2.3423, test_loss: 2.3568]\n",
      "Whave ind owe faperees phels tho gourt shar ifginghe tomast thad thane, the there fill andad payt the  \n",
      "\n",
      "[15m 34s (1150 23%) train loss: 2.3269, test_loss: 2.3296]\n",
      "Whar la dos you yomerrathe thaten?\n",
      "Ger hord welece are dooe se;\n",
      "\n",
      "lothest are qhe'd soon mice haks pich \n",
      "\n",
      "[16m 26s (1200 24%) train loss: 2.3119, test_loss: 2.2994]\n",
      "Whase thirt berime soay, onghef dor wourn hind sth bat Isoft the et wond thest thas is the so not to f \n",
      "\n",
      "[17m 18s (1250 25%) train loss: 2.2963, test_loss: 2.3199]\n",
      "Whim\n",
      "\n",
      "TOLOONES:\n",
      "I En hint anethe wolg;e;\n",
      "CI nive his \n",
      "or hir he hasis fale dith ping tar kats thour we \n",
      "\n",
      "[18m 11s (1300 26%) train loss: 2.2919, test_loss: 2.2982]\n",
      "What wis:\n",
      "And, noth, sortaat shat our ural, wir are0 you wime, notr go nid he eigisto beer, youst yand \n",
      "\n",
      "[19m 2s (1350 27%) train loss: 2.2778, test_loss: 2.2861]\n",
      "Whame\n",
      "Thot ar line?\n",
      "\n",
      "Dol when that.\n",
      "Ho that the bood there sener\n",
      "Cour de me sof houl the tberes I hom  \n",
      "\n",
      "[19m 55s (1400 28%) train loss: 2.2516, test_loss: 2.2748]\n",
      "Whit\n",
      "Asser aiveds fe gase;\n",
      "Woul.\n",
      "Ano sof land wnor you he my shay urts.\n",
      "o, fat lea/ he stert shou entl \n",
      "\n",
      "[20m 47s (1450 28%) train loss: 2.2439, test_loss: 2.2393]\n",
      "Wh.\n",
      "\n",
      "AEOA:\n",
      "I hing till thithew thou poth sos his, rowile fase co dith yout wisere thard pra@spe sonth  \n",
      "\n",
      "[21m 40s (1500 30%) train loss: 2.2193, test_loss: 2.2451]\n",
      "What be int in dill thein well butsMand touslly, seler mads sarth, in whay bear? urtent srimees'k nous \n",
      "\n",
      "[22m 32s (1550 31%) train loss: 2.2212, test_loss: 2.2123]\n",
      "Whill the ford are hing age cares bery of ferthe herstrendichec to tnowel\n",
      "\n",
      "I\u000b",
      "EO:\n",
      "Ano  and ans thin wor \n",
      "\n",
      "[23m 24s (1600 32%) train loss: 2.2100, test_loss: 2.2102]\n",
      "Whi\n",
      "Bud pobminn so kproithee.\n",
      "\n",
      "LESLALONIUS:\n",
      "That hing my for my hatt ciour to to you pgard dis wean an \n",
      "\n",
      "[24m 15s (1650 33%) train loss: 2.1964, test_loss: 2.1928]\n",
      "Whavet arercand st bor and in.\n",
      "\n",
      "CERDULIO:\n",
      "Hy he now thes hood rowing frelas sharsswere sond barles yor \n",
      "\n",
      "[25m 8s (1700 34%) train loss: 2.1808, test_loss: 2.1857]\n",
      "Whe speance londelp toon, bishand, ans distarse eneatht the veple wipar to peiche sour wing ir anow an \n",
      "\n",
      "[26m 1s (1750 35%) train loss: 2.1624, test_loss: 2.1938]\n",
      "Whe moad; whe beall will yor!\n",
      "Corsur thing our and and mangous sing and and nnom the imil som wo ford  \n",
      "\n",
      "[26m 53s (1800 36%) train loss: 2.1625, test_loss: 2.1754]\n",
      "Whous thes one shard pithu; muriff the gemy thater oun well this leresty the the sell and wich matt on \n",
      "\n",
      "[27m 43s (1850 37%) train loss: 2.1485, test_loss: 2.1727]\n",
      "Whe, I my\n",
      "BeennodM\n",
      "We harang\n",
      "Whou thet nor to bool fir the thas me frot the kin.\n",
      "\n",
      "WARI MEROLK:\n",
      "Le are  \n",
      "\n",
      "[28m 36s (1900 38%) train loss: 2.1570, test_loss: 2.1709]\n",
      "Whe gum butekt this ha moke he ho whve me ow; be tow:\n",
      "Thut be you thouv thour mach porle, you, I wick  \n",
      "\n",
      "[29m 28s (1950 39%) train loss: 2.1226, test_loss: 2.1459]\n",
      "Whes and and, the, ho falt snathg pirs'g.\n",
      "The ind word all andseresser's. shave furs the you the bretv \n",
      "\n",
      "[30m 21s (2000 40%) train loss: 2.1249, test_loss: 2.1412]\n",
      "Whe drondy, but hot and as in and more you be condoen and be of thn be kerie, ordy are\n",
      " ase pathe digh \n",
      "\n",
      "[31m 8s (2050 41%) train loss: 2.1004, test_loss: 2.1404]\n",
      "Whay if.\n",
      "All be o, made ame the thy, pobfe, fur you no hard.\n",
      "\n",
      "FoI sey uldiser thy\n",
      "To meed ther lord ro \n",
      "\n",
      "[31m 49s (2100 42%) train loss: 2.1146, test_loss: 2.1312]\n",
      "Wherd the fot momenter that br conef'll me betrowny putry coup.\n",
      "\n",
      "CRand romeg.\n",
      "\n",
      "PORDOLBO:\n",
      "Wordes, hing  \n",
      "\n",
      "[32m 30s (2150 43%) train loss: 2.1048, test_loss: 2.1124]\n",
      "What  hould maed thath\n",
      "Bur.\n",
      "\n",
      "YUET:\n",
      "Thou home were bove and my and hast mening to wich wiik ther he0d h \n",
      "\n",
      "[33m 18s (2200 44%) train loss: 2.0916, test_loss: 2.1032]\n",
      "Whates the will wime soman, chath.\n",
      "\n",
      "CAMENTENGLOM:\n",
      "Dorion for proves thou sease him.\n",
      "\n",
      "Nond:\n",
      "Thour your  \n",
      "\n",
      "[34m 11s (2250 45%) train loss: 2.0829, test_loss: 2.1284]\n",
      "Whrave wat the be such an nan his son thene wholbenis brame toe to hing me badeny.\n",
      "\n",
      "IKE ARD EMERO:\n",
      "I s \n",
      "\n",
      "[35m 3s (2300 46%) train loss: 2.0808, test_loss: 2.0906]\n",
      "Whim to good; deater wame and glemill, and, naths sost what lorlds ald sieng the trave live heak, be,\n",
      " \n",
      "\n",
      "[35m 54s (2350 47%) train loss: 2.0708, test_loss: 2.0837]\n",
      "Wha the that laven\n",
      "Fis llablise my hetreds have my the herpsser a not, uss gode.\n",
      "\n",
      "YWORLANI YnRLOLIO:\n",
      "A \n",
      "\n",
      "[36m 46s (2400 48%) train loss: 2.0565, test_loss: 2.0854]\n",
      "Whou roth the hath and will dear chonow derte of ald aly uel vingush the ko caith thou hengce Maned ha \n",
      "\n",
      "[37m 37s (2450 49%) train loss: 2.0557, test_loss: 2.0863]\n",
      "Wherd hen cospere.\n",
      "\n",
      "HERDO:\n",
      "he me me, te best thot mall have that wake in simen.\n",
      "this theremour the are \n",
      "\n",
      "[38m 29s (2500 50%) train loss: 2.0467, test_loss: 2.0742]\n",
      "Why:\n",
      "The peredy att for theer ar nay chant it ne the our this to shard, a the cordiness aghar prive ar \n",
      "\n",
      "[39m 20s (2550 51%) train loss: 2.0493, test_loss: 2.0832]\n",
      "What sount to the Aopplandere dave itend\n",
      "My sorimen arce,\n",
      "Hom to wather:\n",
      "Bettet limes, sous oull,\n",
      "And  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40m 11s (2600 52%) train loss: 2.0526, test_loss: 2.0816]\n",
      "What,\n",
      "It lath to duantigher and wet the this diching of thit mone urenge, to she will.\n",
      "\n",
      "FORENER:\n",
      "Vous? \n",
      "\n",
      "[41m 3s (2650 53%) train loss: 2.0375, test_loss: 2.0634]\n",
      "Whom ast the flay soak past dithy lankieg, to aad but not the will such as hare hay\n",
      "Soulleathre\n",
      "Chat l \n",
      "\n",
      "[41m 54s (2700 54%) train loss: 2.0236, test_loss: 2.0688]\n",
      "Whou be and shought cep there the rey it that has you wrane: thust un'd so kuth,\n",
      "If to bel the with, t \n",
      "\n",
      "[42m 47s (2750 55%) train loss: 2.0231, test_loss: 2.0234]\n",
      "Whus deer will aplien is heit wes dearp's thour sord, and that enthour 'hath the how prame reat concen \n",
      "\n",
      "[43m 39s (2800 56%) train loss: 2.0117, test_loss: 2.0534]\n",
      "Whim?\n",
      "Thee west this yes.\n",
      "\n",
      "SAUED:\n",
      "Nor:\n",
      "Noring them wath, I have hid beto gread, I and of when the shel \n",
      "\n",
      "[44m 32s (2850 56%) train loss: 1.9935, test_loss: 2.0449]\n",
      "Whce you hict in we preet lat for wise pore me all |hit is all the cras.\n",
      "\n",
      "BICUS:\n",
      "And shat she cowsce b \n",
      "\n",
      "[45m 24s (2900 57%) train loss: 1.9901, test_loss: 2.0388]\n",
      "Whind bathen and so bead?\n",
      "\n",
      "ALARS:\n",
      "What me heon biglike the with you, him and flome\n",
      "\n",
      "Lik Cand take hers \n",
      "\n",
      "[46m 17s (2950 59%) train loss: 1.9755, test_loss: 2.0504]\n",
      "Whis and cint shang?\n",
      "Whe enthene ford\n",
      "Ha tood I wither but masing and you bure to theurud eres mast an \n",
      "\n",
      "[47m 10s (3000 60%) train loss: 1.9910, test_loss: 1.9937]\n",
      "Whe sillow.\n",
      "\n",
      "Fere is pive are rore stords, and that the a go way\n",
      "to no I have lifion mathin the\n",
      "And Go \n",
      "\n",
      "[48m 2s (3050 61%) train loss: 1.9842, test_loss: 2.0318]\n",
      "Whis and with hins soulfore in fairel, nom bact chareal, lignongelf ue kes were teer that of the there \n",
      "\n",
      "[48m 54s (3100 62%) train loss: 1.9507, test_loss: 2.0448]\n",
      "Whut hee.\n",
      "\n",
      "SUCES:\n",
      "Forist\n",
      "Ant deither chave ald come pear lord ba down and frost heant thou me.\n",
      "\n",
      "ILINI: \n",
      "\n",
      "[49m 49s (3150 63%) train loss: 1.9658, test_loss: 2.0142]\n",
      "Whase sash I noth, hopreding,\n",
      "And that ere you cardir,\n",
      "O were heast  with me:\n",
      "The grapliren theatfol g \n",
      "\n",
      "[50m 42s (3200 64%) train loss: 1.9780, test_loss: 1.9989]\n",
      "Wh lomp or of and to borrom Parear,\n",
      "And am a firsinof with never\n",
      "And the sard withisses hast\n",
      "Cempollis \n",
      "\n",
      "[51m 31s (3250 65%) train loss: 1.9594, test_loss: 2.0171]\n",
      "Whe Light me  ghid he countiet yes 'Non lithing yee of dand\n",
      "Whou neen for not shath lood's we vich ant \n",
      "\n",
      "[52m 12s (3300 66%) train loss: 1.9324, test_loss: 2.0025]\n",
      "Whate that unden serce as hateart.\n",
      "Ce jord,\n",
      "Wall were shand he wird do wiry.\n",
      "\n",
      "YORD DICHARD:\n",
      "Singe's\n",
      "An \n",
      "\n",
      "[52m 52s (3350 67%) train loss: 1.9218, test_loss: 2.0296]\n",
      "Who ford\n",
      "Cotrist.\n",
      "\n",
      "Get it your's thou bet we fancenor your for tall to bead,'d me suvent of fare homer \n",
      "\n",
      "[53m 32s (3400 68%) train loss: 1.9628, test_loss: 1.9805]\n",
      "Whe have with to thers'd court me ack theen not is cay eave here! I froks you hang sere.\n",
      "\n",
      "ROUCfPr:\n",
      "But \n",
      "\n",
      "[54m 24s (3450 69%) train loss: 1.9242, test_loss: 1.9918]\n",
      "Where y that is ast and a fordeal?\n",
      "\n",
      "LENRCENE:\n",
      "Whour wind aw a more!\n",
      "\n",
      "Yerich of a comann lout salled a  \n",
      "\n",
      "[55m 15s (3500 70%) train loss: 1.9187, test_loss: 1.9823]\n",
      "Whend hen\n",
      "He do compert this wheer that pricking an metale\n",
      "Loor the sout me worderved your Marpie this \n",
      "\n",
      "[56m 6s (3550 71%) train loss: 1.9267, test_loss: 1.9914]\n",
      "Whrre befall betites rearls forther's hes to foreaty.\n",
      "\n",
      "BELARDII:\n",
      "Nor peadl's gurt not the for of come, \n",
      "\n",
      "[56m 57s (3600 72%) train loss: 1.9048, test_loss: 1.9853]\n",
      "Where the madened at your of he pears: my nerethes\n",
      "With the wis the .\n",
      "Rome of well will the buled lath \n",
      "\n",
      "[57m 48s (3650 73%) train loss: 1.9083, test_loss: 1.9828]\n",
      "Who bromed, lear in arcurt there;\n",
      "And the do with are gery at ploperan of I have of agation,\n",
      "I libs! y \n",
      "\n",
      "[58m 39s (3700 74%) train loss: 1.9224, test_loss: 1.9906]\n",
      "Why mien you hored not shass,\n",
      "Thou had and and and to thou deings,\n",
      "He will ha me me hear whenelo sake  \n",
      "\n",
      "[59m 30s (3750 75%) train loss: 1.9429, test_loss: 1.9755]\n",
      "Whe ploed is not shave in the sall prope of have with derse the with thee orke werther tied hes that V \n",
      "\n",
      "[60m 21s (3800 76%) train loss: 1.8948, test_loss: 1.9693]\n",
      "Whe mever:\n",
      "Foring a well cistrunter minjols, shalirt wely this take to in there apone:\n",
      "And youllown,\n",
      "A \n",
      "\n",
      "[61m 12s (3850 77%) train loss: 1.9065, test_loss: 1.9561]\n",
      "Wht the of giting broth the conger of am soul for it that fort toun, sigh dood and have;\n",
      "\n",
      "KING RICHENC \n",
      "\n",
      "[62m 3s (3900 78%) train loss: 1.9051, test_loss: 1.9632]\n",
      "Whe dood'd not chound us you, with hear my hear is dome,\n",
      "Sences' atour doucasoef to of herean sire and \n",
      "\n",
      "[62m 54s (3950 79%) train loss: 1.9070, test_loss: 1.9936]\n",
      "Whis badeur lurses love\n",
      "To mane and the soll be he cure tide to I deat\n",
      "He auphter thou sair be the you \n",
      "\n",
      "[63m 44s (4000 80%) train loss: 1.8649, test_loss: 1.9643]\n",
      "Whe hich not gauasise when hour.\n",
      "\n",
      "Bevinh, I wall, I gay, whome, theerle firpaly,\n",
      "I mair his brechir fr \n",
      "\n",
      "[64m 35s (4050 81%) train loss: 1.8893, test_loss: 1.9491]\n",
      "Why mording, be of there that with the mentar\n",
      "Breme the there restrantth wet a sourst to the may, I bu \n",
      "\n",
      "[65m 26s (4100 82%) train loss: 1.8775, test_loss: 1.9473]\n",
      "Whe caliof your and the suncens\n",
      "I were you the will shall thued,\n",
      "There novish:\n",
      "\n",
      "DOREO:\n",
      "Cowngo;\n",
      "Thy s t \n",
      "\n",
      "[66m 16s (4150 83%) train loss: 1.8731, test_loss: 1.9573]\n",
      "Whis spar wan at thes shy lace,\n",
      "How strethin: you.\n",
      "\n",
      "PRIOUS:\n",
      "I\n",
      "Thon mace of my stien Morder, of your no \n",
      "\n",
      "[67m 8s (4200 84%) train loss: 1.8665, test_loss: 1.9426]\n",
      "Whthin a tear at have the have whom thou framist the no so be how;\n",
      "And fferite, of fuch you?\n",
      "\n",
      "Fir verv \n",
      "\n",
      "[67m 59s (4250 85%) train loss: 1.8593, test_loss: 1.9475]\n",
      "What seave is the e voak the stile of iny me honet.\n",
      "Wrich, for there yew to concounter the kills, to c \n",
      "\n",
      "[68m 51s (4300 86%) train loss: 1.8695, test_loss: 1.9348]\n",
      "Whou their and whil thou pelloy.\n",
      "\n",
      "CUCHUMERTIA:\n",
      "Forld offell porrud of of so sollang blongret demant af \n",
      "\n",
      "[69m 39s (4350 87%) train loss: 1.8592, test_loss: 1.9657]\n",
      "Whe ow ingous\n",
      "Civent ablences the shourd his ofes me naty kinggo, be shown and thou toll with our for  \n",
      "\n",
      "[70m 23s (4400 88%) train loss: 1.8635, test_loss: 1.9360]\n",
      "Who kinter\n",
      "Marsing vizes.\n",
      "\n",
      "NI{KE VINCENUS:\n",
      "Whilly.\n",
      "\n",
      "LORIO:\n",
      "And all your I sounce brought but and the w \n",
      "\n",
      "[71m 4s (4450 89%) train loss: 1.8651, test_loss: 1.9420]\n",
      "Whis scorcion:\n",
      "Thy my tore to ent,\n",
      "You sace\n",
      "He on their will if the and meith hore cay thence is our s \n",
      "\n",
      "[71m 48s (4500 90%) train loss: 1.8543, test_loss: 1.9259]\n",
      "Whis lack not custer sourselrow\n",
      "To have and shorm, you sreat the fortone is to cond out he warnet, of  \n",
      "\n",
      "[72m 36s (4550 91%) train loss: 1.8497, test_loss: 1.9260]\n",
      "Whe hath me and of sard; the sore all and shave me of utingle, surst send that sive it all the gives f \n",
      "\n",
      "[73m 25s (4600 92%) train loss: 1.8748, test_loss: 1.9395]\n",
      "Who lay.\n",
      "\n",
      "TUCEND:\n",
      "Nour bakeard bore surmer'l.\n",
      "\n",
      "KING EDWBRONLIO:\n",
      "Have as this knean's his for in and ag \n",
      "\n",
      "[74m 14s (4650 93%) train loss: 1.8539, test_loss: 1.9263]\n",
      "Whe sween him.\n",
      "\n",
      "ESERONG RICHARD II:\n",
      "O, bestrees, untend and farder.\n",
      "\n",
      "BOLUS:\n",
      "Her aft make shall in a, b \n",
      "\n",
      "[75m 1s (4700 94%) train loss: 1.8436, test_loss: 1.9097]\n",
      "Whe sisy ery here scoud,\n",
      "Which he bidime is naing of ece farres your mentents thee, yot can the offelo \n",
      "\n",
      "[75m 49s (4750 95%) train loss: 1.8539, test_loss: 1.9162]\n",
      "Whis it this some of in your good, to fait you fold the temen;\n",
      "The ligrante in is mees! his of this Mi \n",
      "\n",
      "[76m 38s (4800 96%) train loss: 1.8229, test_loss: 1.9468]\n",
      "Whim,\n",
      "He thou cold in the put good your not fiellon:\n",
      "For and not of that store\n",
      "And still.\n",
      "\n",
      "DUKE NINIUS \n",
      "\n",
      "[77m 26s (4850 97%) train loss: 1.8043, test_loss: 1.9392]\n",
      "Whis are here bread kee,\n",
      "And as hearrs main farrioth, and forts our to the out he will as I all as its \n",
      "\n",
      "[78m 15s (4900 98%) train loss: 1.8159, test_loss: 1.9105]\n",
      "Where the'ther:\n",
      "His protreft,\n",
      "Et Ed the sery flead.\n",
      "\n",
      "Vore mane had bedered fike be his not, weather to \n",
      "\n",
      "[79m 4s (4950 99%) train loss: 1.8312, test_loss: 1.9241]\n",
      "Whe both with cland my stand that this shorld I upse that ue would to him in the car; is the soungs,\n",
      "A \n",
      "\n",
      "[79m 53s (5000 100%) train loss: 1.8202, test_loss: 1.9241]\n",
      "Whe wisher good kings.\n",
      "\n",
      "RICHARD II:\n",
      "Will than insick for try prroundy,\n",
      "As the was to great here in tha \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "# torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2df9185c1f0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then would sifford far I ray we of effand 'Ton are uncrown's dobler tort, though that dost me, sarding sworks cant this bo tronkess are you more compoling the fiehtred thy know soing her here have pear if, I with thou it bull shall to hath for shis hath reather.\n",
      "\n",
      "Secome may to out our all's Sented like your from he more a perste, to 'tis cope my nor the fadle let, and we to of all, heir sin?\n",
      "\n",
      "There's it a pray for tare or me on, the all as their for glords gounk, be frumy her to the kind a breast word the some, aw and be,\n",
      "Sich, ouk band is this therselfor: I speepile in and the dody think be for before hold you dous his vooting awws try besty to the for thech to himbuld; for it and the pray.\n",
      "\n",
      "MERCUTIO:\n",
      "Theling of were of poodiony and from me so much an our it that is prove be have his the Moress\n",
      "Thar him to have pance,\n",
      "My have this bast op a on of me lame me fate worke the may mine menece the all pithigue all hatcome tite, blow that my horress.\n",
      "\n",
      "LAUTERTIUS:\n",
      "I thier for may thee to the th\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gru, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = gru.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = gru(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = gru(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.0001\n",
    "model_type = 'gru'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(gru, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = gru.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = gru(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gru, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    \n",
    "    #init_hidden, zero_grad\n",
    "    #loop\n",
    "    #backward, step, outside loop\n",
    "    \n",
    "    #loop 0 to chunk_len\n",
    "    #   output, hidden = \n",
    "    \n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    \n",
    "    hidden = gru.init_hidden(batch_size)\n",
    "    gru.zero_grad()\n",
    "    #print(chunk_len, batch_size, hidden.size())\n",
    "    for i in range(chunk_len):\n",
    "        output, hidden = gru(input[:, i], hidden)\n",
    "        \n",
    "    #need to expand the dimensions of x, where the dimension are (sequence length, batch size, hidden)\n",
    "        loss += criterion(output.view(batch_size,-1), target[:, i])\n",
    "    #loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss.data.item() / chunk_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 52s (50 1%) train loss: 4.3647, test_loss: 4.3654]\n",
      "Wh{lc!8Mthn{Gd_j\"Ur{\n",
      "__QbTZ\\0;&\u000b",
      "[<3tMiMx.|V8ZrFxuhX\u000b",
      "p\"wI#$\"rJ)ED]5\f",
      "!R,VJe691wy$Ynn8+[bYsvQ#<lsb|l.7v1\t \n",
      "\n",
      "[1m 47s (100 2%) train loss: 3.6525, test_loss: 3.6748]\n",
      "WhsYPLbrWX.BfAK(}r*]\f",
      ",^\n",
      "NJ1!4*3namfGn0hBxz 5ionayeaG msshl.fltn )sls lls~\n",
      "ho!. st heoss ltoUe du^`' n~ \n",
      "\n",
      "[2m 40s (150 3%) train loss: 3.3160, test_loss: 3.3343]\n",
      "Whih*af s ln e  tcdnId i\n",
      ",t ? huagtGd s to lho.d sctg,rl dmases t oadiit len^fsie wde eeaeee sl le\n",
      "oot \n",
      "\n",
      "[3m 34s (200 4%) train loss: 3.1957, test_loss: 3.1827]\n",
      "Whtmv aa'itr oms il diw\n",
      ",aweu imr`aens  'sn eacsgatnn\n",
      ".,i f  sl  mutuastms oumrece so mras thtas :ten  \n",
      "\n",
      "[4m 28s (250 5%) train loss: 3.1032, test_loss: 3.0817]\n",
      "R7M.ox\n",
      "Zj\"EB~yUZw+'qA6Ka1P-):gprosd 'm\n",
      "o sy aci'et, ed adis \n",
      "\n",
      ".9 laehaei\n",
      "es d avetue `the s ore e\n",
      " \n",
      "\n",
      "[5m 23s (300 6%) train loss: 2.9780, test_loss: 3.0023]\n",
      "Whyapnin are thimyneo, tneais th an s aleiehaomt se anctmy pscuoatues ai-iiKae bidbu aerio dthare t wn \n",
      "\n",
      "[6m 17s (350 7%) train loss: 2.8747, test_loss: 2.9084]\n",
      "Whe:swt t shnt tiirt thteo aut thhes ae fasef lor foerRd oonn ihte ey ho> d sf itd\n",
      "Wg\n",
      "\n",
      "P_hf amouw ot l \n",
      "\n",
      "[7m 11s (400 8%) train loss: 2.7938, test_loss: 2.8175]\n",
      "Wh& meif by mt.rr hcereie mat gdae py Ahvede idi,\n",
      "Ke; meuas af thenhyn iwhe mio mak hoanc greo hn ton  \n",
      "\n",
      "[8m 4s (450 9%) train loss: 2.7263, test_loss: 2.7440]\n",
      "Wh\n",
      " fal me con riadhi w th thrard, ave tou nal ctoinerlthak thof s t awo' outif y thar gfalr tol arsw\n",
      " \n",
      "\n",
      "[8m 59s (500 10%) train loss: 2.6653, test_loss: 2.7173]\n",
      "Whaf witm frewos sioute rou dy ts bnamha?esmM areit inoue akid piuE mert thean ty ous\n",
      "U sths aofd pary \n",
      "\n",
      "[9m 53s (550 11%) train loss: 2.6346, test_loss: 2.6007]\n",
      "Wh.\n",
      "AL\n",
      "Cthe fis hatithe tnounlrg\n",
      "Wy folated:\n",
      "\n",
      "Tinde, an clthlesiUn the reunit\n",
      "\n",
      "\n",
      "ITI watin aI gthe ave, \n",
      "\n",
      "[10m 46s (600 12%) train loss: 2.5628, test_loss: 2.5772]\n",
      "Wht, ill hond wo\n",
      "\n",
      "ACy fwes\n",
      "Mumel wor ises y nowy inat th tos vereoh me ne at og a8y ronerlin to had to \n",
      "\n",
      "[11m 38s (650 13%) train loss: 2.5155, test_loss: 2.5136]\n",
      "Whesrt, there thimithe fool fofers merow mon d mele irst beant og cecous neatsh heky serre anathe fso  \n",
      "\n",
      "[12m 31s (700 14%) train loss: 2.4707, test_loss: 2.4789]\n",
      "u.\n",
      "\n",
      "Bo the har me mnac'd coondcou oul thine his felle thedh\n",
      "\n",
      "Wronthead where douref bes waluss ath  \n",
      "\n",
      "[13m 25s (750 15%) train loss: 2.4248, test_loss: 2.4366]\n",
      "Whes, to ther wot ot fourced Tos ood hiptheus thor heo -lisn aun withe cof thut pou kedegh to alr mor  \n",
      "\n",
      "[14m 17s (800 16%) train loss: 2.4035, test_loss: 2.3952]\n",
      "Whill erme soat thim hate, ad beint fho weind thay, it sered thee ware of ad anes won lto, weut thavrg \n",
      "\n",
      "[15m 9s (850 17%) train loss: 2.3736, test_loss: 2.3898]\n",
      "WhoTh ame rome miwo mamand gouith so at od rerol, cory:\n",
      "Sele the mat thad dom likif:\n",
      "eme I ind thatte  \n",
      "\n",
      "[16m 0s (900 18%) train loss: 2.3363, test_loss: 2.3531]\n",
      "WhMpbe pgand fow hand houn sante, he berer seres she wimur hacigndist ourd hed you the he an ryou thic \n",
      "\n",
      "[16m 54s (950 19%) train loss: 2.3365, test_loss: 2.3350]\n",
      "Wher saerast me I ponel the ine, ther he morelnat pereeld sour deriy and thern fory afed theret pers,  \n",
      "\n",
      "[17m 47s (1000 20%) train loss: 2.2994, test_loss: 2.3111]\n",
      "Wh, the sor thes dous to hind And whinithe anfest gore the meingas ther me yas,\n",
      "Her bris moth ghande e \n",
      "\n",
      "[18m 39s (1050 21%) train loss: 2.2819, test_loss: 2.2899]\n",
      "Whinee as.\n",
      "O:\n",
      "ees yous, gong these megewiss wour ssis ig hino mome des conrermer'd bare amt thent worm \n",
      "\n",
      "[19m 31s (1100 22%) train loss: 2.2527, test_loss: 2.2392]\n",
      "Whtind rouse lildema mead\n",
      "Bur'sen, worcme he and the ave stey ofon you the wom blorty sly taise me iir \n",
      "\n",
      "[20m 25s (1150 23%) train loss: 2.2611, test_loss: 2.2527]\n",
      "Whey you by the and yunce Yoind,\n",
      "\n",
      "Huthuno lous thot've thas to sears lesters it pourg hif boore bore.\n",
      " \n",
      "\n",
      "[21m 19s (1200 24%) train loss: 2.2288, test_loss: 2.2606]\n",
      "Whif by hat serle me seams ol?\n",
      "To here, my the liee sord are hin\n",
      "Foer eres.\n",
      "\n",
      "ALOONINO:\n",
      "Wherd that the  \n",
      "\n",
      "[22m 12s (1250 25%) train loss: 2.2030, test_loss: 2.2303]\n",
      "Wheif ther that ile hit hornstein ad elly wo lent hey, her hain!\n",
      "Mertanchot palld ils thich om wik:\n",
      "No \n",
      "\n",
      "[23m 7s (1300 26%) train loss: 2.2082, test_loss: 2.1921]\n",
      "Whice, what or cirly the nand to he withe ghast fyor pure thean sill that lice she dou. Marst some sou \n",
      "\n",
      "[24m 2s (1350 27%) train loss: 2.1707, test_loss: 2.1807]\n",
      "Whind the shere, ke last to hous my soune dais her ive the seam shalt an wer and the tary\n",
      "PORG ONE HFa \n",
      "\n",
      "[24m 55s (1400 28%) train loss: 2.1435, test_loss: 2.1659]\n",
      "Whthe in the vemonondim hory your mend\n",
      "I ang sot thom me you hatake the thet were ur hat the, I thams  \n",
      "\n",
      "[25m 48s (1450 28%) train loss: 2.1364, test_loss: 2.1488]\n",
      "Whe of so hen hamte the and the aw, ily us endror, Fay ere sead withe my geis he be fansss your angen  \n",
      "\n",
      "[26m 42s (1500 30%) train loss: 2.1141, test_loss: 2.1400]\n",
      "Whive shisly feith uwveller melus not and whe be pinceres aflind.\n",
      "\n",
      "LEECENREN LIYY:\n",
      "No dowl Trege sbode \n",
      "\n",
      "[27m 34s (1550 31%) train loss: 2.1228, test_loss: 2.1382]\n",
      "Whed tre ane love love heor: wadis for here and hats ton the sror is sher ance prordst swand to dore?\n",
      " \n",
      "\n",
      "[28m 28s (1600 32%) train loss: 2.0905, test_loss: 2.1032]\n",
      "Whand the buter are ir mator wide mond nom I loud but heardy the of shom of in four:\n",
      "Len.\n",
      "\n",
      "NoU UC:\n",
      "In  \n",
      "\n",
      "[29m 22s (1650 33%) train loss: 2.0775, test_loss: 2.0941]\n",
      "Wher'd wiflads my this this bromen bespreather there lother hering.\n",
      "\n",
      "ATYBLO:\n",
      "Therand and give lenst th \n",
      "\n",
      "[30m 16s (1700 34%) train loss: 2.0418, test_loss: 2.0999]\n",
      "Whing cen your gowh and with the the a theat gageay.\n",
      "\n",
      "PALO:\n",
      "U anderer wing-the to in with this you but \n",
      "\n",
      "[31m 11s (1750 35%) train loss: 2.0767, test_loss: 2.0816]\n",
      "Whe. Sor and thin much prothers filst grace osout, cere: be heve hate and,\n",
      "Anscince my the wath thinge \n",
      "\n",
      "[32m 5s (1800 36%) train loss: 2.0376, test_loss: 2.0700]\n",
      "Wher and my im had deest your conore hir the sother grot the mast tir Congosoth thes thear beath the t \n",
      "\n",
      "[32m 48s (1850 37%) train loss: 2.0269, test_loss: 2.0453]\n",
      "Wher the corderd.\n",
      "\n",
      "KING RIRCINIUS:\n",
      "The cowning, you tread the speeds,\n",
      "That with aling to me hould kenc \n",
      "\n",
      "[33m 38s (1900 38%) train loss: 2.0206, test_loss: 2.0515]\n",
      "Whis cume the the lare my in a my ham by the cure, daben;\n",
      "And the the thou the of ing thee ther a men. \n",
      "\n",
      "[34m 20s (1950 39%) train loss: 1.9958, test_loss: 2.0312]\n",
      "When my wide we lill thee goodise of ald the cand hon you rome:\n",
      "and in mace his be thearts yous dease, \n",
      "\n",
      "[35m 2s (2000 40%) train loss: 1.9932, test_loss: 2.0198]\n",
      "Wheralt the whoh to do evous cormers all and thee, I gork to be maseut met with and whas you of that m \n",
      "\n",
      "[35m 41s (2050 41%) train loss: 1.9860, test_loss: 2.0259]\n",
      "Whe stow, be thas me hive of theart, and but fore.\n",
      "\n",
      "PENGCELA:\n",
      "Round mach the pore the hive Bures. Lang \n",
      "\n",
      "[36m 22s (2100 42%) train loss: 1.9738, test_loss: 2.0051]\n",
      "Wher I with loult ere ore seefur read all the here to to me comast you, thees sood thear the tray is h \n",
      "\n",
      "[37m 2s (2150 43%) train loss: 1.9599, test_loss: 2.0339]\n",
      "Whish nore daver paruply brome his wire, wibot berive, that seld sword's betsel wand he has thee disha \n",
      "\n",
      "[37m 42s (2200 44%) train loss: 1.9935, test_loss: 2.0114]\n",
      "Why so prout one?\n",
      "His arther you quence ale his to chonone my wing,\n",
      "With stor dearce om wal that the m \n",
      "\n",
      "[38m 22s (2250 45%) train loss: 1.9699, test_loss: 1.9875]\n",
      "Whiruns thuth to eir heirs:\n",
      "The cries froud bright of thee as ftitch to forsed a gave to frestone, at  \n",
      "\n",
      "[39m 2s (2300 46%) train loss: 1.9377, test_loss: 2.0074]\n",
      "Whink, it tight day, ming and enwerdel? I shumsend shaunling to roove we faright louds and he for to d \n",
      "\n",
      "[39m 41s (2350 47%) train loss: 1.9385, test_loss: 1.9879]\n",
      "Whing of dow hims, not arman theapting wight\n",
      "For for with one my sury, there to thy have do my I be co \n",
      "\n",
      "[40m 21s (2400 48%) train loss: 1.9017, test_loss: 1.9632]\n",
      "Whis that wall, hee shoy so there my see vone, of errous mared,\n",
      "Whis lammes,\n",
      "Alder that the and thing  \n",
      "\n",
      "[41m 0s (2450 49%) train loss: 1.9392, test_loss: 1.9597]\n",
      "Wher thou his the hild but not ceel is he fall to kin there allands all lords and all iprimon blooth'l \n",
      "\n",
      "[41m 40s (2500 50%) train loss: 1.9252, test_loss: 1.9767]\n",
      "Why of, this leven,\n",
      "Ourd he marsmore our spink weilraty shoartiter the hought bly sharces.\n",
      "\n",
      "QERTIS:\n",
      "Go \n",
      "\n",
      "[42m 20s (2550 51%) train loss: 1.9022, test_loss: 1.9625]\n",
      "Whad your here and the there alnoth fair to bath ever.\n",
      "\n",
      "PELWIA:\n",
      "Dright me as his buple is lord.\n",
      "\n",
      "CORDO \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43m 0s (2600 52%) train loss: 1.9003, test_loss: 1.9529]\n",
      "Wher wore and Hall to may thus in but ounay lisher, fathim them come of the fureds be hapward erech wi \n",
      "\n",
      "[43m 40s (2650 53%) train loss: 1.8809, test_loss: 1.9515]\n",
      "Whould be whose a be cursaly your hear\n",
      "Astain the king well binther by leth prome blay: with must to n \n",
      "\n",
      "[44m 19s (2700 54%) train loss: 1.8692, test_loss: 1.9186]\n",
      "Whoth well me be to perear had be live there solt shiment, thee you paun'd I vandes\n",
      "Lous lord!\n",
      "Seel yo \n",
      "\n",
      "[44m 59s (2750 55%) train loss: 1.8509, test_loss: 1.9435]\n",
      "When her have, for them enersed of\n",
      "Ray stebll, more to he heargry curch ou do\n",
      "I weart! I'll shood was  \n",
      "\n",
      "[45m 38s (2800 56%) train loss: 1.8497, test_loss: 1.9473]\n",
      "Where so your my thimen, antieforat of eneed not am be their'd decharmand thew tome booken, mitine pra \n",
      "\n",
      "[46m 18s (2850 56%) train loss: 1.8354, test_loss: 1.8989]\n",
      "Whem'd the hast hearthy; so beathed pamus with not\n",
      "I way the cing you not as me,\n",
      "And so she with ston  \n",
      "\n",
      "[46m 58s (2900 57%) train loss: 1.8523, test_loss: 1.8945]\n",
      "Whind my man bears;\n",
      "I mill all feat, for herse, and lord niught you to say, made\n",
      "It Xmongsefor a your  \n",
      "\n",
      "[47m 39s (2950 59%) train loss: 1.8404, test_loss: 1.9096]\n",
      "Wher thing thou gool and sterman with be the that cantery.\n",
      "\n",
      "GIONE:\n",
      "There!\n",
      "Be a pence sirlison her me m \n",
      "\n",
      "[48m 23s (3000 60%) train loss: 1.8363, test_loss: 1.9032]\n",
      "Whis!\n",
      "\n",
      "GNORD:\n",
      "O with way, who pain so the here be king at\n",
      "Lother the light than stlust and all the sol \n",
      "\n",
      "[49m 9s (3050 61%) train loss: 1.8377, test_loss: 1.9156]\n",
      "Whild sike to my attion the can to abeder, consently kain,\n",
      "My him them'd so paked and there but with a \n",
      "\n",
      "[49m 53s (3100 62%) train loss: 1.8228, test_loss: 1.8885]\n",
      "Whnemish teart seet infinine a lives,\n",
      "I om case the gaster is antist?\n",
      "\n",
      "Fuling.\n",
      "\n",
      "VOMERD:\n",
      "And doraing th \n",
      "\n",
      "[50m 36s (3150 63%) train loss: 1.8387, test_loss: 1.9189]\n",
      "Wher and thein the oft my golds,\n",
      "And do momplage!\n",
      "\n",
      "CAMORCHERD IAS:\n",
      "That he wert is to you shall love.\n",
      " \n",
      "\n",
      "[51m 20s (3200 64%) train loss: 1.7844, test_loss: 1.8956]\n",
      "Whome and in they by have his.\n",
      "\n",
      "RICINIA:\n",
      "Elower chare good I wast the pries on a port thee will wheref \n",
      "\n",
      "[52m 0s (3250 65%) train loss: 1.8031, test_loss: 1.8728]\n",
      "Wher turpily toor, and ride me facks thou him you his knait I hopes.\n",
      "\n",
      "HENING RICILINIUS:\n",
      "Whold thanss  \n",
      "\n",
      "[52m 40s (3300 66%) train loss: 1.7894, test_loss: 1.8834]\n",
      "Whatuin; and nem the\n",
      "and propose cassess portoo he trives pering\n",
      "inight, then look Lont to gurt sir.\n",
      "\n",
      " \n",
      "\n",
      "[53m 22s (3350 67%) train loss: 1.8161, test_loss: 1.9004]\n",
      "Whow slay.\n",
      "\n",
      "DUCESTENS:\n",
      "For the sees,\n",
      "A sinfory hear men and never yourstry if the of to the from you t \n",
      "\n",
      "[54m 5s (3400 68%) train loss: 1.7754, test_loss: 1.8854]\n",
      "Whine.\n",
      "\n",
      "DUKE IThe lept be will I artireain of the sirest: and have contory.\n",
      "'To ming here to thy like  \n",
      "\n",
      "[54m 46s (3450 69%) train loss: 1.7884, test_loss: 1.8528]\n",
      "Wher he dones, Sonder in the preace,\n",
      "And will name the she our his shall to eveng his that you interd  \n",
      "\n",
      "[55m 27s (3500 70%) train loss: 1.7514, test_loss: 1.8465]\n",
      "Whath grind of that with hours on thy lord,\n",
      "Far it the graines\n",
      "To be the were the here siscuse:\n",
      "Bemen  \n",
      "\n",
      "[56m 9s (3550 71%) train loss: 1.7778, test_loss: 1.8966]\n",
      "Wher call am it will teart to hey wars of the will glack,\n",
      "Yor come or our had fow me of pall her\n",
      "with  \n",
      "\n",
      "[56m 52s (3600 72%) train loss: 1.7805, test_loss: 1.8592]\n",
      "Wher gisting him andy gind the srair.\n",
      "\n",
      "KIN ELIZINGRIUS:\n",
      "To of trure\n",
      "And to knoul to his to thy grave f \n",
      "\n",
      "[57m 33s (3650 73%) train loss: 1.7641, test_loss: 1.8564]\n",
      "Who death enders for aplooding your dright be her heres brity!\n",
      "\n",
      "BUCINIUS:\n",
      "That the sut the hersion:\n",
      "An \n",
      "\n",
      "[58m 16s (3700 74%) train loss: 1.7544, test_loss: 1.8390]\n",
      "When, more, in the were the deser's must upon\n",
      "And with thus to mond!\n",
      "He prind son share I lorded ore b \n",
      "\n",
      "[58m 59s (3750 75%) train loss: 1.7580, test_loss: 1.8509]\n",
      "Whem have vieser\n",
      "Come, you shollant have words;\n",
      "Where sindought can pance and go seise.\n",
      "The crown to y \n",
      "\n",
      "[59m 39s (3800 76%) train loss: 1.7565, test_loss: 1.8632]\n",
      "What:\n",
      "The one for we pricited for hear, end,\n",
      "In are the look, them voice? be.\n",
      "\n",
      "CULIT:\n",
      "Printixst and I' \n",
      "\n",
      "[60m 19s (3850 77%) train loss: 1.7351, test_loss: 1.8553]\n",
      "Whirce? I courdion, she stand mage of to fist my.\n",
      "\n",
      "MERY ENTIUS:\n",
      "And the your would the such to forenin \n",
      "\n",
      "[61m 0s (3900 78%) train loss: 1.7494, test_loss: 1.8521]\n",
      "Whath, brod slair and are to have of and good:\n",
      "And thee you for in that it not for in ented to?\n",
      "\n",
      "AUCIF \n",
      "\n",
      "[61m 41s (3950 79%) train loss: 1.7341, test_loss: 1.8513]\n",
      "Whild, thy laukets and will\n",
      "In that, and the shall which an you do'll him'd\n",
      "Ol not blead a slowled whi \n",
      "\n",
      "[62m 22s (4000 80%) train loss: 1.7386, test_loss: 1.8287]\n",
      "Wht: jarwing.\n",
      "\n",
      "BY:\n",
      "\n",
      "GLOUCESTRARD:\n",
      "The Richink our sun'd on the sweeth and foar that the beshill.\n",
      "\n",
      "BUCI \n",
      "\n",
      "[63m 5s (4050 81%) train loss: 1.7242, test_loss: 1.8445]\n",
      "Wher and they to is of you?\n",
      "\n",
      "Mreto terve one at the dayals the head your creidy:\n",
      "A shave it of shall g \n",
      "\n",
      "[63m 45s (4100 82%) train loss: 1.7103, test_loss: 1.8231]\n",
      "Wher truts you having of us to be at my loves;\n",
      "This porcenty of buld thou got upon upon wand:\n",
      "Hat abta \n",
      "\n",
      "[64m 26s (4150 83%) train loss: 1.7098, test_loss: 1.8625]\n",
      "Whise the good, and to fie!\n",
      "\n",
      "LEMARD:\n",
      "But with your some be be good theres; the come, not so make your  \n",
      "\n",
      "[65m 6s (4200 84%) train loss: 1.7096, test_loss: 1.8286]\n",
      "Whomor, you.\n",
      "\n",
      "QUEEN OF OF MARDIO:\n",
      "My brasuren the is, the set the's these God the cornous some:\n",
      "How th \n",
      "\n",
      "[65m 46s (4250 85%) train loss: 1.7141, test_loss: 1.8576]\n",
      "Wher mark and thou day,\n",
      "Come shall a lide, this stworn, come our have in the dight adod in as my were  \n",
      "\n",
      "[66m 26s (4300 86%) train loss: 1.7258, test_loss: 1.8235]\n",
      "Whee strayed and the porty;\n",
      "Mysidered beg is all honoursecth;\n",
      "The plostanger bear in this all shemect  \n",
      "\n",
      "[67m 11s (4350 87%) train loss: 1.7199, test_loss: 1.8383]\n",
      "Whent as that he bart of that may a mone,\n",
      "ople me let but of the armenger or mance,\n",
      "The lark the ville \n",
      "\n",
      "[67m 59s (4400 88%) train loss: 1.6999, test_loss: 1.8952]\n",
      "Wher theak with nast the saided\n",
      "That of me you say that hear to the beode hath moford\n",
      "To see fortin he \n",
      "\n",
      "[68m 46s (4450 89%) train loss: 1.6844, test_loss: 1.8300]\n",
      "Who'll good an eur more,\n",
      "Or now deise at king sleeming and offires;\n",
      "To sich on your giste.\n",
      "\n",
      "MENENIUS:\n",
      " \n",
      "\n",
      "[69m 26s (4500 90%) train loss: 1.6950, test_loss: 1.8072]\n",
      "Where man fornemit. Nome not gring one be crease and; I toursely fathems.\n",
      "\n",
      "QUEEN EDOF:\n",
      "Thand for which \n",
      "\n",
      "[70m 7s (4550 91%) train loss: 1.6876, test_loss: 1.8045]\n",
      "Whard up mets agenceties in the griess\n",
      "pastor's he lord, but this live agincest\n",
      "To the nuge.\n",
      "\n",
      "Be:\n",
      "O sp \n",
      "\n",
      "[70m 50s (4600 92%) train loss: 1.6830, test_loss: 1.8321]\n",
      "Wher, where a thantharn seet him and mare I reasleass:\n",
      "For hold that see the fing like though: will go \n",
      "\n",
      "[71m 32s (4650 93%) train loss: 1.6868, test_loss: 1.8080]\n",
      "Whand look'st my porcoust the preged:\n",
      "Reak friends pience to with my. For mastarve?\n",
      "A the daster lets: \n",
      "\n",
      "[72m 13s (4700 94%) train loss: 1.6949, test_loss: 1.8382]\n",
      "Whacy Of: which make, him could thing!\n",
      "\n",
      "ARDIO:\n",
      "That I though even many his dale and from the cust and  \n",
      "\n",
      "[72m 55s (4750 95%) train loss: 1.6698, test_loss: 1.8131]\n",
      "Whard thy love from world for the world;\n",
      "And in I with not to ming the lord!\n",
      "\n",
      "RUCERDIO:\n",
      "I speater up y \n",
      "\n",
      "[73m 37s (4800 96%) train loss: 1.6908, test_loss: 1.8155]\n",
      "Wher I semorted; the heirsed\n",
      "Or curn themseld, do eving quickess.\n",
      "\n",
      "BUCHARDIV:\n",
      "I't shries with the wauc \n",
      "\n",
      "[74m 22s (4850 97%) train loss: 1.6705, test_loss: 1.7864]\n",
      "Wher cat time on your fings and they off me\n",
      "Hath earry seepas, and thysary\n",
      "Yeath as rediry.\n",
      "\n",
      "VINGARET: \n",
      "\n",
      "[75m 16s (4900 98%) train loss: 1.6958, test_loss: 1.8213]\n",
      "Whee.\n",
      "\n",
      "GLOUDINE:\n",
      "And me do who agious sing'd horought of the know?\n",
      "\n",
      "POLING:\n",
      "Cone, she cale the would u \n",
      "\n",
      "[76m 15s (4950 99%) train loss: 1.6466, test_loss: 1.8124]\n",
      "Whides! God buldenge's of of him and trey that\n",
      "We hands tell can a should hendring charks. Not appity  \n",
      "\n",
      "[77m 17s (5000 100%) train loss: 1.6378, test_loss: 1.8250]\n",
      "Whild so love to see.\n",
      "\n",
      "Spayour,\n",
      "For God sir, suse's dead, I them, be constlear to me made the bow\n",
      "coun \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gru = GRU(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(gru, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(gru, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(gru, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2df918fd3a0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The come,\n",
      "And on theneswall of this swaillow?\n",
      "\n",
      "BENEL IPINI:\n",
      "Hanced we with not the coust old with offern thy come senciount dear kingly I the grather do lown it;\n",
      "And be so be he to bedly to here quanters of rowness for thought of fess, lived thee, thin reep his laits and his, is doing dit will have breenthat manigne?\n",
      "Tith thing mondere sand give\n",
      "And on up a dlesp's are you this hamford:\n",
      "I this depomences to saile other dight strifher be wow pronged am the blow\n",
      "Mortion thee piny pese afto; your hore on ross a way from this now as an king; this moran, wedber,\n",
      "The terray shall for the other he suking\n",
      "Well Perpote\n",
      "First say to the cammente.\n",
      "\n",
      "Serith dives is of my comnthat you couttre\n",
      "thou, I ment this here the it the goot bited you our love so in thus not witce a prears hand trim, do do then tr fe told shands in be to the sirst ak good of it your, and now weads upon that enery lo he crastens,\n",
      "Who minging reard, is the tood of reto like their hold am himp gust probes, at this the soy reso now\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
