{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\computer\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\computer\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: EasyProcess in c:\\users\\computer\\anaconda3\\lib\\site-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: future in c:\\users\\computer\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in c:\\users\\computer\\anaconda3\\lib\\site-packages (50.3.2)\n",
      "Requirement already satisfied: ez_setup in c:\\users\\computer\\anaconda3\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\computer\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: opencv-python; extra == \"atari\" in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (4.4.0.46)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in c:\\users\\computer\\anaconda3\\lib\\site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\computer\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\computer\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 247   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 2.0   memory length: 444   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 2   score: 1.0   memory length: 616   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 3   score: 5.0   memory length: 980   epsilon: 1.0    steps: 364    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 4   score: 0.0   memory length: 1103   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 5   score: 0.0   memory length: 1226   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 6   score: 1.0   memory length: 1376   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 7   score: 0.0   memory length: 1498   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 8   score: 2.0   memory length: 1716   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 9   score: 2.0   memory length: 1936   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 10   score: 3.0   memory length: 2182   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.7272727272727273\n",
      "episode: 11   score: 0.0   memory length: 2305   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 12   score: 0.0   memory length: 2428   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 13   score: 1.0   memory length: 2597   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 14   score: 2.0   memory length: 2794   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
      "episode: 15   score: 0.0   memory length: 2917   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 16   score: 2.0   memory length: 3114   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.411764705882353\n",
      "episode: 17   score: 8.0   memory length: 3481   epsilon: 1.0    steps: 367    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 18   score: 4.0   memory length: 3774   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.894736842105263\n",
      "episode: 19   score: 0.0   memory length: 3897   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 20   score: 1.0   memory length: 4047   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.7619047619047619\n",
      "episode: 21   score: 2.0   memory length: 4265   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7727272727272727\n",
      "episode: 22   score: 1.0   memory length: 4416   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7391304347826086\n",
      "episode: 23   score: 0.0   memory length: 4538   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 24   score: 0.0   memory length: 4660   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 25   score: 0.0   memory length: 4782   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
      "episode: 26   score: 1.0   memory length: 4952   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5185185185185186\n",
      "episode: 27   score: 3.0   memory length: 5195   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 28   score: 1.0   memory length: 5363   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5517241379310345\n",
      "episode: 29   score: 1.0   memory length: 5531   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5333333333333334\n",
      "episode: 30   score: 3.0   memory length: 5775   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.5806451612903225\n",
      "episode: 31   score: 1.0   memory length: 5944   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5625\n",
      "episode: 32   score: 3.0   memory length: 6173   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.606060606060606\n",
      "episode: 33   score: 2.0   memory length: 6355   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.6176470588235294\n",
      "episode: 34   score: 1.0   memory length: 6506   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 35   score: 3.0   memory length: 6775   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.6388888888888888\n",
      "episode: 36   score: 3.0   memory length: 7022   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6756756756756757\n",
      "episode: 37   score: 0.0   memory length: 7145   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.631578947368421\n",
      "episode: 38   score: 0.0   memory length: 7268   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5897435897435896\n",
      "episode: 39   score: 4.0   memory length: 7543   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 40   score: 3.0   memory length: 7789   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6829268292682926\n",
      "episode: 41   score: 1.0   memory length: 7939   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 42   score: 1.0   memory length: 8108   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6511627906976745\n",
      "episode: 43   score: 0.0   memory length: 8231   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6136363636363635\n",
      "episode: 44   score: 2.0   memory length: 8428   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6222222222222222\n",
      "episode: 45   score: 1.0   memory length: 8579   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 46   score: 1.0   memory length: 8729   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5957446808510638\n",
      "episode: 47   score: 1.0   memory length: 8880   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 48   score: 0.0   memory length: 9003   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5510204081632653\n",
      "episode: 49   score: 2.0   memory length: 9200   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 50   score: 3.0   memory length: 9444   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 51   score: 4.0   memory length: 9757   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.6346153846153846\n",
      "episode: 52   score: 0.0   memory length: 9880   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6037735849056605\n",
      "episode: 53   score: 3.0   memory length: 10105   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6296296296296295\n",
      "episode: 54   score: 1.0   memory length: 10274   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6181818181818182\n",
      "episode: 55   score: 1.0   memory length: 10445   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6071428571428572\n",
      "episode: 56   score: 3.0   memory length: 10673   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.631578947368421\n",
      "episode: 57   score: 2.0   memory length: 10870   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6379310344827587\n",
      "episode: 58   score: 5.0   memory length: 11194   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.694915254237288\n",
      "episode: 59   score: 0.0   memory length: 11316   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 60   score: 0.0   memory length: 11439   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.639344262295082\n",
      "episode: 61   score: 2.0   memory length: 11637   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6451612903225807\n",
      "episode: 62   score: 0.0   memory length: 11759   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 63   score: 0.0   memory length: 11882   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 2.0   memory length: 12099   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 65   score: 1.0   memory length: 12250   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
      "episode: 66   score: 0.0   memory length: 12373   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5671641791044777\n",
      "episode: 67   score: 2.0   memory length: 12571   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5735294117647058\n",
      "episode: 68   score: 2.0   memory length: 12788   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5797101449275361\n",
      "episode: 69   score: 5.0   memory length: 13137   epsilon: 1.0    steps: 349    lr: 0.0001     evaluation reward: 1.6285714285714286\n",
      "episode: 70   score: 0.0   memory length: 13260   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6056338028169015\n",
      "episode: 71   score: 1.0   memory length: 13430   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5972222222222223\n",
      "episode: 72   score: 3.0   memory length: 13638   epsilon: 1.0    steps: 208    lr: 0.0001     evaluation reward: 1.6164383561643836\n",
      "episode: 73   score: 0.0   memory length: 13761   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5945945945945945\n",
      "episode: 74   score: 2.0   memory length: 13959   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 75   score: 1.0   memory length: 14131   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5921052631578947\n",
      "episode: 76   score: 0.0   memory length: 14254   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 77   score: 0.0   memory length: 14376   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5512820512820513\n",
      "episode: 78   score: 2.0   memory length: 14576   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5569620253164558\n",
      "episode: 79   score: 2.0   memory length: 14773   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5625\n",
      "episode: 80   score: 1.0   memory length: 14942   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 81   score: 1.0   memory length: 15113   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.548780487804878\n",
      "episode: 82   score: 3.0   memory length: 15360   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5662650602409638\n",
      "episode: 83   score: 0.0   memory length: 15482   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5476190476190477\n",
      "episode: 84   score: 1.0   memory length: 15633   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5411764705882354\n",
      "episode: 85   score: 1.0   memory length: 15802   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5348837209302326\n",
      "episode: 86   score: 3.0   memory length: 16049   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5517241379310345\n",
      "episode: 87   score: 0.0   memory length: 16172   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5340909090909092\n",
      "episode: 88   score: 1.0   memory length: 16323   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5280898876404494\n",
      "episode: 89   score: 0.0   memory length: 16445   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.511111111111111\n",
      "episode: 90   score: 0.0   memory length: 16568   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4945054945054945\n",
      "episode: 91   score: 1.0   memory length: 16738   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4891304347826086\n",
      "episode: 92   score: 0.0   memory length: 16861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4731182795698925\n",
      "episode: 93   score: 1.0   memory length: 17030   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4680851063829787\n",
      "episode: 94   score: 0.0   memory length: 17152   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4526315789473685\n",
      "episode: 95   score: 0.0   memory length: 17275   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4375\n",
      "episode: 96   score: 1.0   memory length: 17426   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4329896907216495\n",
      "episode: 97   score: 0.0   memory length: 17549   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4183673469387754\n",
      "episode: 98   score: 2.0   memory length: 17747   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4242424242424243\n",
      "episode: 99   score: 1.0   memory length: 17915   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 100   score: 4.0   memory length: 18177   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 101   score: 1.0   memory length: 18348   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 102   score: 0.0   memory length: 18471   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 103   score: 2.0   memory length: 18690   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 104   score: 2.0   memory length: 18907   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 105   score: 2.0   memory length: 19107   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 106   score: 2.0   memory length: 19305   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 107   score: 2.0   memory length: 19523   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 108   score: 1.0   memory length: 19692   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 109   score: 1.0   memory length: 19861   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 110   score: 2.0   memory length: 20077   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 111   score: 0.0   memory length: 20200   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 112   score: 3.0   memory length: 20447   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 113   score: 3.0   memory length: 20680   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 114   score: 1.0   memory length: 20831   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 115   score: 3.0   memory length: 21096   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 116   score: 0.0   memory length: 21218   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 117   score: 2.0   memory length: 21436   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 118   score: 0.0   memory length: 21559   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 119   score: 0.0   memory length: 21682   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 120   score: 1.0   memory length: 21851   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 121   score: 0.0   memory length: 21974   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 122   score: 0.0   memory length: 22097   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 123   score: 0.0   memory length: 22220   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 124   score: 1.0   memory length: 22371   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 125   score: 3.0   memory length: 22620   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 126   score: 3.0   memory length: 22868   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 127   score: 0.0   memory length: 22991   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 128   score: 2.0   memory length: 23206   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   score: 2.0   memory length: 23404   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 130   score: 1.0   memory length: 23573   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 131   score: 1.0   memory length: 23743   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 132   score: 0.0   memory length: 23866   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 133   score: 0.0   memory length: 23989   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 134   score: 0.0   memory length: 24111   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 135   score: 0.0   memory length: 24234   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 136   score: 2.0   memory length: 24432   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 137   score: 2.0   memory length: 24630   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 138   score: 2.0   memory length: 24848   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 139   score: 1.0   memory length: 25017   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 140   score: 0.0   memory length: 25140   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 141   score: 0.0   memory length: 25263   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 142   score: 1.0   memory length: 25414   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 143   score: 2.0   memory length: 25612   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 144   score: 2.0   memory length: 25812   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 145   score: 0.0   memory length: 25934   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 146   score: 0.0   memory length: 26057   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 147   score: 1.0   memory length: 26229   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 148   score: 2.0   memory length: 26448   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 149   score: 2.0   memory length: 26664   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 150   score: 0.0   memory length: 26786   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 151   score: 2.0   memory length: 26984   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 152   score: 1.0   memory length: 27155   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 153   score: 0.0   memory length: 27277   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 154   score: 2.0   memory length: 27474   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 155   score: 2.0   memory length: 27690   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 156   score: 0.0   memory length: 27813   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 157   score: 3.0   memory length: 28064   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 158   score: 4.0   memory length: 28359   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 159   score: 2.0   memory length: 28539   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 160   score: 2.0   memory length: 28736   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 161   score: 2.0   memory length: 28957   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 162   score: 2.0   memory length: 29175   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 163   score: 2.0   memory length: 29373   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 164   score: 0.0   memory length: 29495   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 165   score: 0.0   memory length: 29618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 166   score: 1.0   memory length: 29786   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 167   score: 1.0   memory length: 29937   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 168   score: 1.0   memory length: 30087   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 169   score: 0.0   memory length: 30210   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 170   score: 2.0   memory length: 30408   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 171   score: 2.0   memory length: 30624   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 172   score: 1.0   memory length: 30793   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 173   score: 1.0   memory length: 30944   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 174   score: 0.0   memory length: 31066   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 175   score: 3.0   memory length: 31333   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 176   score: 3.0   memory length: 31597   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 177   score: 0.0   memory length: 31720   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 178   score: 2.0   memory length: 31917   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 179   score: 2.0   memory length: 32114   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 180   score: 2.0   memory length: 32311   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 181   score: 3.0   memory length: 32558   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 182   score: 0.0   memory length: 32680   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 183   score: 2.0   memory length: 32877   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 184   score: 3.0   memory length: 33085   epsilon: 1.0    steps: 208    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 185   score: 3.0   memory length: 33331   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 186   score: 0.0   memory length: 33453   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 187   score: 1.0   memory length: 33625   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 188   score: 4.0   memory length: 33919   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 189   score: 1.0   memory length: 34069   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 190   score: 0.0   memory length: 34192   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 191   score: 3.0   memory length: 34404   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 192   score: 0.0   memory length: 34527   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 193   score: 0.0   memory length: 34650   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 194   score: 1.0   memory length: 34818   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 195   score: 3.0   memory length: 35043   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 196   score: 0.0   memory length: 35165   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 197   score: 1.0   memory length: 35316   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 198   score: 2.0   memory length: 35515   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 199   score: 0.0   memory length: 35638   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 200   score: 0.0   memory length: 35760   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 201   score: 2.0   memory length: 35978   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 202   score: 1.0   memory length: 36149   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 203   score: 1.0   memory length: 36300   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 204   score: 4.0   memory length: 36596   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 205   score: 2.0   memory length: 36813   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 206   score: 2.0   memory length: 36998   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 207   score: 4.0   memory length: 37273   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 208   score: 1.0   memory length: 37442   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 209   score: 0.0   memory length: 37565   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 210   score: 2.0   memory length: 37763   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 211   score: 2.0   memory length: 37960   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 212   score: 1.0   memory length: 38129   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 213   score: 1.0   memory length: 38299   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 214   score: 3.0   memory length: 38565   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 215   score: 2.0   memory length: 38783   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 216   score: 1.0   memory length: 38934   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 217   score: 0.0   memory length: 39057   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 218   score: 0.0   memory length: 39180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 219   score: 2.0   memory length: 39401   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 220   score: 1.0   memory length: 39552   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 221   score: 1.0   memory length: 39704   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 222   score: 0.0   memory length: 39827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 223   score: 0.0   memory length: 39950   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 224   score: 0.0   memory length: 40073   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 225   score: 1.0   memory length: 40223   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 226   score: 2.0   memory length: 40403   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 227   score: 2.0   memory length: 40623   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 228   score: 0.0   memory length: 40746   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 229   score: 1.0   memory length: 40915   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 230   score: 0.0   memory length: 41038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 231   score: 0.0   memory length: 41160   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 232   score: 3.0   memory length: 41406   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 233   score: 1.0   memory length: 41575   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 234   score: 4.0   memory length: 41872   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 235   score: 2.0   memory length: 42053   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 236   score: 0.0   memory length: 42176   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 237   score: 1.0   memory length: 42345   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 238   score: 1.0   memory length: 42514   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 239   score: 2.0   memory length: 42732   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 240   score: 2.0   memory length: 42950   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 241   score: 1.0   memory length: 43101   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 242   score: 2.0   memory length: 43300   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 243   score: 2.0   memory length: 43518   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 244   score: 2.0   memory length: 43716   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 245   score: 2.0   memory length: 43934   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 246   score: 3.0   memory length: 44164   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 247   score: 3.0   memory length: 44390   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 248   score: 0.0   memory length: 44513   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 249   score: 2.0   memory length: 44730   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 250   score: 0.0   memory length: 44852   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 251   score: 2.0   memory length: 45069   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 252   score: 3.0   memory length: 45315   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 253   score: 2.0   memory length: 45537   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 254   score: 0.0   memory length: 45659   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 255   score: 0.0   memory length: 45782   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 256   score: 1.0   memory length: 45933   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 257   score: 2.0   memory length: 46131   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 258   score: 2.0   memory length: 46328   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 259   score: 2.0   memory length: 46545   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 260   score: 1.0   memory length: 46715   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 261   score: 0.0   memory length: 46837   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 262   score: 1.0   memory length: 46988   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 263   score: 3.0   memory length: 47218   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 264   score: 0.0   memory length: 47341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 265   score: 3.0   memory length: 47587   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 266   score: 0.0   memory length: 47709   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 267   score: 1.0   memory length: 47879   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 268   score: 0.0   memory length: 48001   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 269   score: 0.0   memory length: 48124   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 270   score: 3.0   memory length: 48370   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 271   score: 0.0   memory length: 48493   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 272   score: 1.0   memory length: 48662   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 273   score: 1.0   memory length: 48813   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 274   score: 3.0   memory length: 49059   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 275   score: 2.0   memory length: 49256   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 276   score: 0.0   memory length: 49378   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 277   score: 1.0   memory length: 49547   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 278   score: 1.0   memory length: 49697   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 279   score: 3.0   memory length: 49943   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 280   score: 5.0   memory length: 50308   epsilon: 1.0    steps: 365    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 281   score: 1.0   memory length: 50477   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 282   score: 0.0   memory length: 50599   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 283   score: 1.0   memory length: 50749   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 284   score: 3.0   memory length: 50995   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 285   score: 2.0   memory length: 51212   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 286   score: 0.0   memory length: 51335   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 287   score: 1.0   memory length: 51505   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 288   score: 4.0   memory length: 51803   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 289   score: 2.0   memory length: 52019   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 290   score: 1.0   memory length: 52188   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 291   score: 1.0   memory length: 52358   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 292   score: 0.0   memory length: 52480   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 293   score: 1.0   memory length: 52649   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 294   score: 4.0   memory length: 52945   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 295   score: 1.0   memory length: 53096   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 296   score: 1.0   memory length: 53265   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 297   score: 0.0   memory length: 53387   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 298   score: 1.0   memory length: 53556   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 299   score: 2.0   memory length: 53754   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 300   score: 0.0   memory length: 53877   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 301   score: 2.0   memory length: 54079   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 302   score: 2.0   memory length: 54296   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 303   score: 1.0   memory length: 54446   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 304   score: 2.0   memory length: 54664   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 305   score: 4.0   memory length: 54921   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 306   score: 1.0   memory length: 55072   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 307   score: 2.0   memory length: 55290   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 308   score: 3.0   memory length: 55519   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 309   score: 0.0   memory length: 55642   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 310   score: 1.0   memory length: 55793   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 311   score: 3.0   memory length: 56041   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 312   score: 2.0   memory length: 56241   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 313   score: 2.0   memory length: 56442   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 314   score: 0.0   memory length: 56565   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 315   score: 1.0   memory length: 56734   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 316   score: 4.0   memory length: 57013   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 317   score: 3.0   memory length: 57282   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 318   score: 0.0   memory length: 57405   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 319   score: 1.0   memory length: 57555   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 320   score: 0.0   memory length: 57678   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 321   score: 0.0   memory length: 57800   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 322   score: 2.0   memory length: 57982   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 323   score: 2.0   memory length: 58179   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 324   score: 1.0   memory length: 58331   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 325   score: 0.0   memory length: 58453   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 326   score: 0.0   memory length: 58576   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 327   score: 1.0   memory length: 58748   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 328   score: 1.0   memory length: 58920   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 329   score: 0.0   memory length: 59043   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 330   score: 1.0   memory length: 59194   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 331   score: 0.0   memory length: 59316   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 332   score: 0.0   memory length: 59439   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 333   score: 3.0   memory length: 59707   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 334   score: 2.0   memory length: 59927   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 335   score: 1.0   memory length: 60095   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 336   score: 1.0   memory length: 60263   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 337   score: 1.0   memory length: 60435   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 338   score: 1.0   memory length: 60606   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 339   score: 1.0   memory length: 60757   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 340   score: 3.0   memory length: 60983   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 341   score: 0.0   memory length: 61106   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 342   score: 2.0   memory length: 61303   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 343   score: 1.0   memory length: 61472   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 344   score: 1.0   memory length: 61641   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 345   score: 2.0   memory length: 61859   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 346   score: 0.0   memory length: 61982   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 347   score: 1.0   memory length: 62153   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 348   score: 4.0   memory length: 62410   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 349   score: 0.0   memory length: 62532   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 350   score: 2.0   memory length: 62750   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 351   score: 0.0   memory length: 62873   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 352   score: 2.0   memory length: 63090   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 353   score: 2.0   memory length: 63291   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 354   score: 2.0   memory length: 63489   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 355   score: 0.0   memory length: 63612   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 356   score: 0.0   memory length: 63735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 357   score: 0.0   memory length: 63857   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 358   score: 0.0   memory length: 63979   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 359   score: 0.0   memory length: 64101   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 360   score: 0.0   memory length: 64224   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 361   score: 1.0   memory length: 64395   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 362   score: 4.0   memory length: 64669   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 363   score: 0.0   memory length: 64792   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 364   score: 3.0   memory length: 65038   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 365   score: 1.0   memory length: 65208   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 366   score: 2.0   memory length: 65425   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 367   score: 2.0   memory length: 65643   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 368   score: 5.0   memory length: 65943   epsilon: 1.0    steps: 300    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 369   score: 1.0   memory length: 66112   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 370   score: 5.0   memory length: 66454   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 371   score: 3.0   memory length: 66698   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 372   score: 1.0   memory length: 66868   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 373   score: 0.0   memory length: 66991   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 374   score: 2.0   memory length: 67188   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 375   score: 0.0   memory length: 67311   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 376   score: 2.0   memory length: 67532   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 377   score: 2.0   memory length: 67730   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 378   score: 2.0   memory length: 67930   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 379   score: 2.0   memory length: 68127   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 380   score: 3.0   memory length: 68373   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 381   score: 3.0   memory length: 68603   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 382   score: 2.0   memory length: 68820   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 383   score: 4.0   memory length: 69100   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 384   score: 0.0   memory length: 69223   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 385   score: 2.0   memory length: 69423   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 386   score: 3.0   memory length: 69633   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 387   score: 0.0   memory length: 69755   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 388   score: 1.0   memory length: 69925   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 389   score: 0.0   memory length: 70048   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 390   score: 4.0   memory length: 70322   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 391   score: 1.0   memory length: 70491   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 392   score: 0.0   memory length: 70614   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 393   score: 1.0   memory length: 70764   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 394   score: 0.0   memory length: 70887   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 395   score: 1.0   memory length: 71058   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 396   score: 1.0   memory length: 71227   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 397   score: 0.0   memory length: 71349   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 398   score: 0.0   memory length: 71471   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 399   score: 0.0   memory length: 71594   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 400   score: 1.0   memory length: 71765   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 401   score: 2.0   memory length: 71983   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 402   score: 2.0   memory length: 72200   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 403   score: 2.0   memory length: 72398   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 404   score: 2.0   memory length: 72596   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405   score: 0.0   memory length: 72718   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 406   score: 0.0   memory length: 72841   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 407   score: 2.0   memory length: 73062   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 408   score: 0.0   memory length: 73184   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 409   score: 0.0   memory length: 73306   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 410   score: 0.0   memory length: 73428   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 411   score: 3.0   memory length: 73675   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 412   score: 1.0   memory length: 73826   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 413   score: 3.0   memory length: 74092   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 414   score: 1.0   memory length: 74262   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 415   score: 3.0   memory length: 74535   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 416   score: 1.0   memory length: 74706   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 417   score: 4.0   memory length: 74981   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 418   score: 0.0   memory length: 75103   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 419   score: 4.0   memory length: 75365   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 420   score: 3.0   memory length: 75612   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 421   score: 1.0   memory length: 75782   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 422   score: 3.0   memory length: 76053   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 423   score: 2.0   memory length: 76270   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 424   score: 0.0   memory length: 76392   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 425   score: 1.0   memory length: 76542   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 426   score: 1.0   memory length: 76711   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 427   score: 3.0   memory length: 76957   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 428   score: 2.0   memory length: 77138   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 429   score: 2.0   memory length: 77336   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 430   score: 2.0   memory length: 77554   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 431   score: 2.0   memory length: 77774   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 432   score: 3.0   memory length: 78000   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 433   score: 1.0   memory length: 78151   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 434   score: 1.0   memory length: 78301   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 435   score: 1.0   memory length: 78471   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 436   score: 0.0   memory length: 78593   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 437   score: 2.0   memory length: 78791   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 438   score: 0.0   memory length: 78914   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 439   score: 1.0   memory length: 79083   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 440   score: 2.0   memory length: 79284   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 441   score: 0.0   memory length: 79407   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 442   score: 1.0   memory length: 79558   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 443   score: 2.0   memory length: 79775   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 444   score: 3.0   memory length: 80003   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 445   score: 0.0   memory length: 80126   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 446   score: 1.0   memory length: 80294   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 447   score: 1.0   memory length: 80465   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 448   score: 0.0   memory length: 80587   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 449   score: 0.0   memory length: 80710   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 450   score: 0.0   memory length: 80832   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 451   score: 1.0   memory length: 80983   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 452   score: 3.0   memory length: 81252   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 453   score: 2.0   memory length: 81471   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 454   score: 1.0   memory length: 81621   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 455   score: 0.0   memory length: 81743   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 456   score: 2.0   memory length: 81940   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 457   score: 2.0   memory length: 82137   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 458   score: 3.0   memory length: 82386   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 459   score: 0.0   memory length: 82508   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 460   score: 1.0   memory length: 82677   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 461   score: 0.0   memory length: 82799   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 462   score: 0.0   memory length: 82921   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 463   score: 1.0   memory length: 83072   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 464   score: 4.0   memory length: 83369   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 465   score: 3.0   memory length: 83580   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 466   score: 3.0   memory length: 83807   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 467   score: 2.0   memory length: 84004   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 468   score: 1.0   memory length: 84155   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 469   score: 3.0   memory length: 84386   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 470   score: 4.0   memory length: 84662   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 471   score: 2.0   memory length: 84880   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 472   score: 0.0   memory length: 85003   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 473   score: 5.0   memory length: 85347   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 474   score: 3.0   memory length: 85593   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 475   score: 4.0   memory length: 85886   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 476   score: 2.0   memory length: 86103   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 477   score: 0.0   memory length: 86226   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 478   score: 1.0   memory length: 86396   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 479   score: 0.0   memory length: 86519   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 480   score: 0.0   memory length: 86642   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 481   score: 3.0   memory length: 86888   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 482   score: 3.0   memory length: 87118   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 483   score: 2.0   memory length: 87317   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 484   score: 3.0   memory length: 87543   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 485   score: 3.0   memory length: 87789   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 486   score: 1.0   memory length: 87959   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 487   score: 3.0   memory length: 88190   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 488   score: 2.0   memory length: 88410   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 489   score: 0.0   memory length: 88533   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 490   score: 1.0   memory length: 88704   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 491   score: 1.0   memory length: 88857   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 492   score: 0.0   memory length: 88980   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 493   score: 0.0   memory length: 89103   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 494   score: 1.0   memory length: 89274   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 495   score: 2.0   memory length: 89472   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 496   score: 1.0   memory length: 89641   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 497   score: 1.0   memory length: 89810   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 498   score: 0.0   memory length: 89933   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 499   score: 0.0   memory length: 90056   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 500   score: 3.0   memory length: 90269   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 501   score: 1.0   memory length: 90419   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 502   score: 2.0   memory length: 90617   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 503   score: 0.0   memory length: 90740   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 504   score: 2.0   memory length: 90920   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 505   score: 4.0   memory length: 91229   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 506   score: 3.0   memory length: 91459   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 507   score: 1.0   memory length: 91627   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 508   score: 0.0   memory length: 91749   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 509   score: 1.0   memory length: 91900   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 510   score: 2.0   memory length: 92100   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 511   score: 2.0   memory length: 92315   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 512   score: 1.0   memory length: 92485   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 513   score: 4.0   memory length: 92764   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 514   score: 1.0   memory length: 92932   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 515   score: 1.0   memory length: 93101   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 516   score: 1.0   memory length: 93252   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 517   score: 1.0   memory length: 93422   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 518   score: 2.0   memory length: 93602   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 519   score: 0.0   memory length: 93725   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 520   score: 2.0   memory length: 93943   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 521   score: 3.0   memory length: 94210   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 522   score: 2.0   memory length: 94408   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 523   score: 0.0   memory length: 94530   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 524   score: 0.0   memory length: 94652   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 525   score: 2.0   memory length: 94851   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 526   score: 2.0   memory length: 95048   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 527   score: 1.0   memory length: 95216   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 528   score: 3.0   memory length: 95461   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 529   score: 4.0   memory length: 95755   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 530   score: 2.0   memory length: 95937   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 531   score: 0.0   memory length: 96059   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 532   score: 0.0   memory length: 96182   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 533   score: 2.0   memory length: 96382   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 534   score: 4.0   memory length: 96659   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 535   score: 1.0   memory length: 96828   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 536   score: 3.0   memory length: 97058   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 537   score: 0.0   memory length: 97180   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 538   score: 1.0   memory length: 97351   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 539   score: 1.0   memory length: 97522   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 540   score: 3.0   memory length: 97765   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 541   score: 4.0   memory length: 98021   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 542   score: 2.0   memory length: 98240   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 543   score: 1.0   memory length: 98409   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 544   score: 4.0   memory length: 98704   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 545   score: 2.0   memory length: 98924   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 546   score: 0.0   memory length: 99047   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 547   score: 0.0   memory length: 99169   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 548   score: 1.0   memory length: 99319   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 549   score: 2.0   memory length: 99538   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 550   score: 2.0   memory length: 99735   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 551   score: 1.0   memory length: 99886   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\Desktop\\fall2020\\cs498dl\\assignment5\\memory.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sample = np.array(sample)\n",
      "C:\\Users\\Computer\\Desktop\\fall2020\\cs498dl\\assignment5\\agent.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 552   score: 1.0   memory length: 100055   epsilon: 0.9998891200000024    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 553   score: 0.0   memory length: 100178   epsilon: 0.9996455800000077    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 554   score: 4.0   memory length: 100474   epsilon: 0.9990595000000204    steps: 296    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 555   score: 2.0   memory length: 100672   epsilon: 0.9986674600000289    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 556   score: 0.0   memory length: 100794   epsilon: 0.9984259000000342    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 557   score: 0.0   memory length: 100917   epsilon: 0.9981823600000395    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 558   score: 5.0   memory length: 101244   epsilon: 0.9975349000000535    steps: 327    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 559   score: 1.0   memory length: 101416   epsilon: 0.9971943400000609    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 560   score: 1.0   memory length: 101567   epsilon: 0.9968953600000674    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 561   score: 1.0   memory length: 101736   epsilon: 0.9965607400000747    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 562   score: 2.0   memory length: 101936   epsilon: 0.9961647400000833    steps: 200    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 563   score: 2.0   memory length: 102154   epsilon: 0.9957331000000926    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 564   score: 1.0   memory length: 102306   epsilon: 0.9954321400000992    steps: 152    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 565   score: 0.0   memory length: 102428   epsilon: 0.9951905800001044    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 566   score: 1.0   memory length: 102600   epsilon: 0.9948500200001118    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 567   score: 1.0   memory length: 102750   epsilon: 0.9945530200001182    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 568   score: 0.0   memory length: 102872   epsilon: 0.9943114600001235    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 569   score: 3.0   memory length: 103098   epsilon: 0.9938639800001332    steps: 226    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 570   score: 3.0   memory length: 103323   epsilon: 0.9934184800001429    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 571   score: 5.0   memory length: 103650   epsilon: 0.9927710200001569    steps: 327    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 572   score: 0.0   memory length: 103773   epsilon: 0.9925274800001622    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 573   score: 2.0   memory length: 103971   epsilon: 0.9921354400001707    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 574   score: 0.0   memory length: 104094   epsilon: 0.991891900000176    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 575   score: 2.0   memory length: 104311   epsilon: 0.9914622400001853    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 576   score: 2.0   memory length: 104531   epsilon: 0.9910266400001948    steps: 220    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 577   score: 3.0   memory length: 104760   epsilon: 0.9905732200002046    steps: 229    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 578   score: 5.0   memory length: 105107   epsilon: 0.9898861600002196    steps: 347    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 579   score: 0.0   memory length: 105229   epsilon: 0.9896446000002248    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 580   score: 1.0   memory length: 105380   epsilon: 0.9893456200002313    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 581   score: 1.0   memory length: 105549   epsilon: 0.9890110000002386    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 582   score: 1.0   memory length: 105718   epsilon: 0.9886763800002458    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 583   score: 0.0   memory length: 105841   epsilon: 0.9884328400002511    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 584   score: 1.0   memory length: 106012   epsilon: 0.9880942600002585    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 585   score: 0.0   memory length: 106134   epsilon: 0.9878527000002637    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 586   score: 2.0   memory length: 106352   epsilon: 0.9874210600002731    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 587   score: 3.0   memory length: 106599   epsilon: 0.9869320000002837    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 588   score: 5.0   memory length: 106940   epsilon: 0.9862568200002984    steps: 341    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 589   score: 0.0   memory length: 107063   epsilon: 0.9860132800003036    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 590   score: 0.0   memory length: 107186   epsilon: 0.9857697400003089    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 591   score: 0.0   memory length: 107309   epsilon: 0.9855262000003142    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 592   score: 4.0   memory length: 107603   epsilon: 0.9849440800003268    steps: 294    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 593   score: 1.0   memory length: 107754   epsilon: 0.9846451000003333    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 594   score: 0.0   memory length: 107877   epsilon: 0.9844015600003386    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 595   score: 0.0   memory length: 108000   epsilon: 0.9841580200003439    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 596   score: 4.0   memory length: 108315   epsilon: 0.9835343200003575    steps: 315    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 597   score: 0.0   memory length: 108437   epsilon: 0.9832927600003627    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 598   score: 2.0   memory length: 108637   epsilon: 0.9828967600003713    steps: 200    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 599   score: 0.0   memory length: 108759   epsilon: 0.9826552000003765    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 600   score: 0.0   memory length: 108881   epsilon: 0.9824136400003818    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 601   score: 1.0   memory length: 109032   epsilon: 0.9821146600003883    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 602   score: 1.0   memory length: 109201   epsilon: 0.9817800400003955    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 603   score: 2.0   memory length: 109399   epsilon: 0.981388000000404    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 604   score: 2.0   memory length: 109615   epsilon: 0.9809603200004133    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 605   score: 0.0   memory length: 109738   epsilon: 0.9807167800004186    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 606   score: 2.0   memory length: 109956   epsilon: 0.980285140000428    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 607   score: 0.0   memory length: 110078   epsilon: 0.9800435800004332    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 608   score: 2.0   memory length: 110295   epsilon: 0.9796139200004426    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 609   score: 1.0   memory length: 110466   epsilon: 0.9792753400004499    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 610   score: 1.0   memory length: 110616   epsilon: 0.9789783400004564    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 611   score: 0.0   memory length: 110738   epsilon: 0.9787367800004616    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 612   score: 1.0   memory length: 110910   epsilon: 0.978396220000469    steps: 172    lr: 0.0001     evaluation reward: 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 613   score: 1.0   memory length: 111080   epsilon: 0.9780596200004763    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 614   score: 0.0   memory length: 111202   epsilon: 0.9778180600004815    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 615   score: 0.0   memory length: 111325   epsilon: 0.9775745200004868    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 616   score: 2.0   memory length: 111542   epsilon: 0.9771448600004962    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 617   score: 3.0   memory length: 111790   epsilon: 0.9766538200005068    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 618   score: 2.0   memory length: 111993   epsilon: 0.9762518800005155    steps: 203    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 619   score: 0.0   memory length: 112115   epsilon: 0.9760103200005208    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 620   score: 2.0   memory length: 112335   epsilon: 0.9755747200005302    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 621   score: 2.0   memory length: 112552   epsilon: 0.9751450600005396    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 622   score: 1.0   memory length: 112723   epsilon: 0.9748064800005469    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 623   score: 3.0   memory length: 112970   epsilon: 0.9743174200005575    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 624   score: 3.0   memory length: 113195   epsilon: 0.9738719200005672    steps: 225    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 625   score: 1.0   memory length: 113346   epsilon: 0.9735729400005737    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 626   score: 0.0   memory length: 113468   epsilon: 0.973331380000579    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 627   score: 0.0   memory length: 113591   epsilon: 0.9730878400005842    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 628   score: 2.0   memory length: 113789   epsilon: 0.9726958000005927    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 629   score: 2.0   memory length: 114007   epsilon: 0.9722641600006021    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 630   score: 2.0   memory length: 114205   epsilon: 0.9718721200006106    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 631   score: 0.0   memory length: 114328   epsilon: 0.9716285800006159    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 632   score: 3.0   memory length: 114575   epsilon: 0.9711395200006265    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 633   score: 3.0   memory length: 114824   epsilon: 0.9706465000006372    steps: 249    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 634   score: 0.0   memory length: 114947   epsilon: 0.9704029600006425    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 635   score: 1.0   memory length: 115115   epsilon: 0.9700703200006497    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 636   score: 1.0   memory length: 115266   epsilon: 0.9697713400006562    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 637   score: 2.0   memory length: 115466   epsilon: 0.9693753400006648    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 638   score: 1.0   memory length: 115635   epsilon: 0.9690407200006721    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 639   score: 2.0   memory length: 115833   epsilon: 0.9686486800006806    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 640   score: 2.0   memory length: 116050   epsilon: 0.9682190200006899    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 641   score: 2.0   memory length: 116268   epsilon: 0.9677873800006993    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 642   score: 2.0   memory length: 116465   epsilon: 0.9673973200007078    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 643   score: 0.0   memory length: 116587   epsilon: 0.967155760000713    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 644   score: 2.0   memory length: 116784   epsilon: 0.9667657000007215    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 645   score: 0.0   memory length: 116907   epsilon: 0.9665221600007268    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 646   score: 0.0   memory length: 117030   epsilon: 0.9662786200007321    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 647   score: 1.0   memory length: 117201   epsilon: 0.9659400400007394    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 648   score: 0.0   memory length: 117324   epsilon: 0.9656965000007447    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 649   score: 1.0   memory length: 117495   epsilon: 0.965357920000752    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 650   score: 0.0   memory length: 117618   epsilon: 0.9651143800007573    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 651   score: 1.0   memory length: 117789   epsilon: 0.9647758000007647    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 652   score: 0.0   memory length: 117911   epsilon: 0.9645342400007699    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 653   score: 3.0   memory length: 118182   epsilon: 0.9639976600007816    steps: 271    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 654   score: 0.0   memory length: 118304   epsilon: 0.9637561000007868    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 655   score: 0.0   memory length: 118426   epsilon: 0.9635145400007921    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 656   score: 3.0   memory length: 118675   epsilon: 0.9630215200008028    steps: 249    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 657   score: 1.0   memory length: 118826   epsilon: 0.9627225400008093    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 658   score: 3.0   memory length: 119035   epsilon: 0.9623087200008182    steps: 209    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 659   score: 2.0   memory length: 119233   epsilon: 0.9619166800008268    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 660   score: 0.0   memory length: 119356   epsilon: 0.961673140000832    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 661   score: 3.0   memory length: 119626   epsilon: 0.9611385400008436    steps: 270    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 662   score: 0.0   memory length: 119749   epsilon: 0.9608950000008489    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 663   score: 3.0   memory length: 119998   epsilon: 0.9604019800008596    steps: 249    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 664   score: 1.0   memory length: 120168   epsilon: 0.9600653800008669    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 665   score: 1.0   memory length: 120338   epsilon: 0.9597287800008742    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 666   score: 2.0   memory length: 120518   epsilon: 0.959372380000882    steps: 180    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 667   score: 0.0   memory length: 120641   epsilon: 0.9591288400008873    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 668   score: 2.0   memory length: 120839   epsilon: 0.9587368000008958    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 669   score: 0.0   memory length: 120961   epsilon: 0.958495240000901    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 670   score: 5.0   memory length: 121302   epsilon: 0.9578200600009157    steps: 341    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 671   score: 1.0   memory length: 121453   epsilon: 0.9575210800009222    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 672   score: 1.0   memory length: 121623   epsilon: 0.9571844800009295    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 673   score: 2.0   memory length: 121821   epsilon: 0.956792440000938    steps: 198    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 674   score: 1.0   memory length: 121972   epsilon: 0.9564934600009445    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 675   score: 0.0   memory length: 122095   epsilon: 0.9562499200009498    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 676   score: 1.0   memory length: 122266   epsilon: 0.9559113400009571    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 677   score: 0.0   memory length: 122389   epsilon: 0.9556678000009624    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 678   score: 1.0   memory length: 122558   epsilon: 0.9553331800009697    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 679   score: 0.0   memory length: 122680   epsilon: 0.9550916200009749    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 680   score: 0.0   memory length: 122803   epsilon: 0.9548480800009802    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 681   score: 1.0   memory length: 122953   epsilon: 0.9545510800009867    steps: 150    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 682   score: 0.0   memory length: 123076   epsilon: 0.9543075400009919    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 683   score: 0.0   memory length: 123199   epsilon: 0.9540640000009972    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 684   score: 1.0   memory length: 123350   epsilon: 0.9537650200010037    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 685   score: 0.0   memory length: 123472   epsilon: 0.953523460001009    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 686   score: 1.0   memory length: 123623   epsilon: 0.9532244800010155    steps: 151    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 687   score: 2.0   memory length: 123824   epsilon: 0.9528265000010241    steps: 201    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 688   score: 0.0   memory length: 123946   epsilon: 0.9525849400010293    steps: 122    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 689   score: 1.0   memory length: 124117   epsilon: 0.9522463600010367    steps: 171    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 690   score: 0.0   memory length: 124240   epsilon: 0.952002820001042    steps: 123    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 691   score: 1.0   memory length: 124409   epsilon: 0.9516682000010492    steps: 169    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 692   score: 0.0   memory length: 124531   epsilon: 0.9514266400010545    steps: 122    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 693   score: 1.0   memory length: 124703   epsilon: 0.9510860800010619    steps: 172    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 694   score: 0.0   memory length: 124826   epsilon: 0.9508425400010672    steps: 123    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 695   score: 3.0   memory length: 125074   epsilon: 0.9503515000010778    steps: 248    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 696   score: 3.0   memory length: 125340   epsilon: 0.9498248200010893    steps: 266    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 697   score: 5.0   memory length: 125684   epsilon: 0.949143700001104    steps: 344    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 698   score: 3.0   memory length: 125932   epsilon: 0.9486526600011147    steps: 248    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 699   score: 1.0   memory length: 126102   epsilon: 0.948316060001122    steps: 170    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 700   score: 3.0   memory length: 126368   epsilon: 0.9477893800011334    steps: 266    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 701   score: 0.0   memory length: 126490   epsilon: 0.9475478200011387    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 702   score: 0.0   memory length: 126612   epsilon: 0.9473062600011439    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 703   score: 2.0   memory length: 126809   epsilon: 0.9469162000011524    steps: 197    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 704   score: 3.0   memory length: 127062   epsilon: 0.9464152600011633    steps: 253    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 705   score: 0.0   memory length: 127184   epsilon: 0.9461737000011685    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 706   score: 1.0   memory length: 127355   epsilon: 0.9458351200011759    steps: 171    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 707   score: 0.0   memory length: 127478   epsilon: 0.9455915800011812    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 708   score: 1.0   memory length: 127647   epsilon: 0.9452569600011884    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 709   score: 3.0   memory length: 127875   epsilon: 0.9448055200011982    steps: 228    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 710   score: 4.0   memory length: 128174   epsilon: 0.9442135000012111    steps: 299    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 711   score: 2.0   memory length: 128371   epsilon: 0.9438234400012195    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 712   score: 1.0   memory length: 128522   epsilon: 0.943524460001226    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 713   score: 0.0   memory length: 128644   epsilon: 0.9432829000012313    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 714   score: 1.0   memory length: 128796   epsilon: 0.9429819400012378    steps: 152    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 715   score: 1.0   memory length: 128948   epsilon: 0.9426809800012443    steps: 152    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 716   score: 0.0   memory length: 129070   epsilon: 0.9424394200012496    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 717   score: 1.0   memory length: 129221   epsilon: 0.9421404400012561    steps: 151    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 718   score: 0.0   memory length: 129343   epsilon: 0.9418988800012613    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 719   score: 1.0   memory length: 129511   epsilon: 0.9415662400012685    steps: 168    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 720   score: 1.0   memory length: 129661   epsilon: 0.941269240001275    steps: 150    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 721   score: 0.0   memory length: 129784   epsilon: 0.9410257000012803    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 722   score: 0.0   memory length: 129906   epsilon: 0.9407841400012855    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 723   score: 1.0   memory length: 130074   epsilon: 0.9404515000012927    steps: 168    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 724   score: 1.0   memory length: 130225   epsilon: 0.9401525200012992    steps: 151    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 725   score: 0.0   memory length: 130347   epsilon: 0.9399109600013045    steps: 122    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 726   score: 2.0   memory length: 130565   epsilon: 0.9394793200013138    steps: 218    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 727   score: 1.0   memory length: 130735   epsilon: 0.9391427200013212    steps: 170    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 728   score: 5.0   memory length: 131080   epsilon: 0.938459620001336    steps: 345    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 729   score: 1.0   memory length: 131250   epsilon: 0.9381230200013433    steps: 170    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 730   score: 1.0   memory length: 131418   epsilon: 0.9377903800013505    steps: 168    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 731   score: 2.0   memory length: 131615   epsilon: 0.937400320001359    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 732   score: 1.0   memory length: 131765   epsilon: 0.9371033200013654    steps: 150    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 733   score: 0.0   memory length: 131887   epsilon: 0.9368617600013707    steps: 122    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 734   score: 4.0   memory length: 132201   epsilon: 0.9362400400013842    steps: 314    lr: 0.0001     evaluation reward: 1.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 735   score: 0.0   memory length: 132324   epsilon: 0.9359965000013895    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 736   score: 3.0   memory length: 132550   epsilon: 0.9355490200013992    steps: 226    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 737   score: 3.0   memory length: 132782   epsilon: 0.9350896600014091    steps: 232    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 738   score: 2.0   memory length: 133000   epsilon: 0.9346580200014185    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 739   score: 1.0   memory length: 133151   epsilon: 0.934359040001425    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 740   score: 0.0   memory length: 133274   epsilon: 0.9341155000014303    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 741   score: 1.0   memory length: 133443   epsilon: 0.9337808800014376    steps: 169    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 742   score: 0.0   memory length: 133566   epsilon: 0.9335373400014428    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 743   score: 5.0   memory length: 133907   epsilon: 0.9328621600014575    steps: 341    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 744   score: 2.0   memory length: 134087   epsilon: 0.9325057600014652    steps: 180    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 745   score: 1.0   memory length: 134259   epsilon: 0.9321652000014726    steps: 172    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 746   score: 3.0   memory length: 134485   epsilon: 0.9317177200014823    steps: 226    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 747   score: 1.0   memory length: 134635   epsilon: 0.9314207200014888    steps: 150    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 748   score: 0.0   memory length: 134757   epsilon: 0.931179160001494    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 749   score: 3.0   memory length: 135022   epsilon: 0.9306544600015054    steps: 265    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 750   score: 0.0   memory length: 135145   epsilon: 0.9304109200015107    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 751   score: 1.0   memory length: 135314   epsilon: 0.930076300001518    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 752   score: 1.0   memory length: 135485   epsilon: 0.9297377200015253    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 753   score: 0.0   memory length: 135608   epsilon: 0.9294941800015306    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 754   score: 0.0   memory length: 135731   epsilon: 0.9292506400015359    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 755   score: 0.0   memory length: 135853   epsilon: 0.9290090800015411    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 756   score: 1.0   memory length: 136004   epsilon: 0.9287101000015476    steps: 151    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 757   score: 3.0   memory length: 136233   epsilon: 0.9282566800015575    steps: 229    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 758   score: 0.0   memory length: 136355   epsilon: 0.9280151200015627    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 759   score: 1.0   memory length: 136527   epsilon: 0.9276745600015701    steps: 172    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 760   score: 1.0   memory length: 136697   epsilon: 0.9273379600015774    steps: 170    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 761   score: 2.0   memory length: 136918   epsilon: 0.9269003800015869    steps: 221    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 762   score: 0.0   memory length: 137040   epsilon: 0.9266588200015922    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 763   score: 3.0   memory length: 137288   epsilon: 0.9261677800016028    steps: 248    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 764   score: 2.0   memory length: 137486   epsilon: 0.9257757400016113    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 765   score: 0.0   memory length: 137609   epsilon: 0.9255322000016166    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 766   score: 1.0   memory length: 137778   epsilon: 0.9251975800016239    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 767   score: 1.0   memory length: 137950   epsilon: 0.9248570200016313    steps: 172    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 768   score: 2.0   memory length: 138135   epsilon: 0.9244907200016392    steps: 185    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 769   score: 0.0   memory length: 138257   epsilon: 0.9242491600016445    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 770   score: 2.0   memory length: 138454   epsilon: 0.9238591000016529    steps: 197    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 771   score: 2.0   memory length: 138671   epsilon: 0.9234294400016623    steps: 217    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 772   score: 1.0   memory length: 138840   epsilon: 0.9230948200016695    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 773   score: 0.0   memory length: 138962   epsilon: 0.9228532600016748    steps: 122    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 774   score: 1.0   memory length: 139113   epsilon: 0.9225542800016813    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 775   score: 1.0   memory length: 139264   epsilon: 0.9222553000016878    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 776   score: 1.0   memory length: 139415   epsilon: 0.9219563200016943    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 777   score: 0.0   memory length: 139538   epsilon: 0.9217127800016995    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 778   score: 2.0   memory length: 139754   epsilon: 0.9212851000017088    steps: 216    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 779   score: 1.0   memory length: 139905   epsilon: 0.9209861200017153    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 780   score: 0.0   memory length: 140028   epsilon: 0.9207425800017206    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 781   score: 1.0   memory length: 140179   epsilon: 0.9204436000017271    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 782   score: 1.0   memory length: 140330   epsilon: 0.9201446200017336    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 783   score: 3.0   memory length: 140595   epsilon: 0.919619920001745    steps: 265    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 784   score: 1.0   memory length: 140764   epsilon: 0.9192853000017522    steps: 169    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 785   score: 1.0   memory length: 140915   epsilon: 0.9189863200017587    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 786   score: 2.0   memory length: 141095   epsilon: 0.9186299200017665    steps: 180    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 787   score: 3.0   memory length: 141362   epsilon: 0.9181012600017779    steps: 267    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 788   score: 6.0   memory length: 141700   epsilon: 0.9174320200017925    steps: 338    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 789   score: 2.0   memory length: 141897   epsilon: 0.9170419600018009    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 790   score: 3.0   memory length: 142165   epsilon: 0.9165113200018125    steps: 268    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 791   score: 1.0   memory length: 142317   epsilon: 0.916210360001819    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 792   score: 1.0   memory length: 142489   epsilon: 0.9158698000018264    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 793   score: 3.0   memory length: 142719   epsilon: 0.9154144000018363    steps: 230    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 794   score: 2.0   memory length: 142935   epsilon: 0.9149867200018456    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 795   score: 3.0   memory length: 143183   epsilon: 0.9144956800018562    steps: 248    lr: 0.0001     evaluation reward: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 796   score: 0.0   memory length: 143306   epsilon: 0.9142521400018615    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 797   score: 2.0   memory length: 143507   epsilon: 0.9138541600018701    steps: 201    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 798   score: 0.0   memory length: 143629   epsilon: 0.9136126000018754    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 799   score: 1.0   memory length: 143798   epsilon: 0.9132779800018826    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 800   score: 3.0   memory length: 144026   epsilon: 0.9128265400018925    steps: 228    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 801   score: 0.0   memory length: 144148   epsilon: 0.9125849800018977    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 802   score: 3.0   memory length: 144395   epsilon: 0.9120959200019083    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 803   score: 3.0   memory length: 144624   epsilon: 0.9116425000019182    steps: 229    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 804   score: 1.0   memory length: 144793   epsilon: 0.9113078800019254    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 805   score: 4.0   memory length: 145068   epsilon: 0.9107633800019372    steps: 275    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 806   score: 3.0   memory length: 145333   epsilon: 0.9102386800019486    steps: 265    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 807   score: 1.0   memory length: 145484   epsilon: 0.9099397000019551    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 808   score: 0.0   memory length: 145607   epsilon: 0.9096961600019604    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 809   score: 0.0   memory length: 145729   epsilon: 0.9094546000019657    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 810   score: 0.0   memory length: 145852   epsilon: 0.9092110600019709    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 811   score: 2.0   memory length: 146049   epsilon: 0.9088210000019794    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 812   score: 1.0   memory length: 146200   epsilon: 0.9085220200019859    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 813   score: 1.0   memory length: 146370   epsilon: 0.9081854200019932    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 814   score: 3.0   memory length: 146596   epsilon: 0.9077379400020029    steps: 226    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 815   score: 4.0   memory length: 146892   epsilon: 0.9071518600020156    steps: 296    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 816   score: 0.0   memory length: 147015   epsilon: 0.9069083200020209    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 817   score: 2.0   memory length: 147213   epsilon: 0.9065162800020294    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 818   score: 0.0   memory length: 147335   epsilon: 0.9062747200020347    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 819   score: 1.0   memory length: 147486   epsilon: 0.9059757400020412    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 820   score: 1.0   memory length: 147656   epsilon: 0.9056391400020485    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 821   score: 6.0   memory length: 148030   epsilon: 0.9048986200020646    steps: 374    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 822   score: 1.0   memory length: 148181   epsilon: 0.904599640002071    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 823   score: 0.0   memory length: 148304   epsilon: 0.9043561000020763    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 824   score: 2.0   memory length: 148502   epsilon: 0.9039640600020848    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 825   score: 1.0   memory length: 148654   epsilon: 0.9036631000020914    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 826   score: 3.0   memory length: 148880   epsilon: 0.9032156200021011    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 827   score: 0.0   memory length: 149002   epsilon: 0.9029740600021063    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 828   score: 7.0   memory length: 149250   epsilon: 0.902483020002117    steps: 248    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 829   score: 0.0   memory length: 149373   epsilon: 0.9022394800021223    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 830   score: 2.0   memory length: 149571   epsilon: 0.9018474400021308    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 831   score: 0.0   memory length: 149694   epsilon: 0.9016039000021361    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 832   score: 2.0   memory length: 149875   epsilon: 0.9012455200021439    steps: 181    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 833   score: 1.0   memory length: 150026   epsilon: 0.9009465400021504    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 834   score: 1.0   memory length: 150195   epsilon: 0.9006119200021576    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 835   score: 1.0   memory length: 150365   epsilon: 0.9002753200021649    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 836   score: 2.0   memory length: 150583   epsilon: 0.8998436800021743    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 837   score: 4.0   memory length: 150877   epsilon: 0.8992615600021869    steps: 294    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 838   score: 1.0   memory length: 151048   epsilon: 0.8989229800021943    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 839   score: 1.0   memory length: 151199   epsilon: 0.8986240000022008    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 840   score: 0.0   memory length: 151321   epsilon: 0.898382440002206    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 841   score: 1.0   memory length: 151472   epsilon: 0.8980834600022125    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 842   score: 3.0   memory length: 151719   epsilon: 0.8975944000022231    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 843   score: 0.0   memory length: 151842   epsilon: 0.8973508600022284    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 844   score: 1.0   memory length: 151992   epsilon: 0.8970538600022349    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 845   score: 2.0   memory length: 152190   epsilon: 0.8966618200022434    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 846   score: 0.0   memory length: 152313   epsilon: 0.8964182800022487    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 847   score: 0.0   memory length: 152436   epsilon: 0.8961747400022539    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 848   score: 2.0   memory length: 152654   epsilon: 0.8957431000022633    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 849   score: 3.0   memory length: 152922   epsilon: 0.8952124600022748    steps: 268    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 850   score: 4.0   memory length: 153237   epsilon: 0.8945887600022884    steps: 315    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 851   score: 3.0   memory length: 153463   epsilon: 0.8941412800022981    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 852   score: 2.0   memory length: 153678   epsilon: 0.8937155800023073    steps: 215    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 853   score: 0.0   memory length: 153800   epsilon: 0.8934740200023126    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 854   score: 2.0   memory length: 154016   epsilon: 0.8930463400023219    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 855   score: 2.0   memory length: 154214   epsilon: 0.8926543000023304    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 856   score: 0.0   memory length: 154336   epsilon: 0.8924127400023356    steps: 122    lr: 0.0001     evaluation reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 857   score: 3.0   memory length: 154604   epsilon: 0.8918821000023471    steps: 268    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 858   score: 3.0   memory length: 154871   epsilon: 0.8913534400023586    steps: 267    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 859   score: 1.0   memory length: 155042   epsilon: 0.891014860002366    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 860   score: 4.0   memory length: 155303   epsilon: 0.8904980800023772    steps: 261    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 861   score: 1.0   memory length: 155453   epsilon: 0.8902010800023836    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 862   score: 1.0   memory length: 155622   epsilon: 0.8898664600023909    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 863   score: 0.0   memory length: 155745   epsilon: 0.8896229200023962    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 864   score: 2.0   memory length: 155924   epsilon: 0.8892685000024039    steps: 179    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 865   score: 2.0   memory length: 156121   epsilon: 0.8888784400024123    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 866   score: 0.0   memory length: 156244   epsilon: 0.8886349000024176    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 867   score: 1.0   memory length: 156394   epsilon: 0.8883379000024241    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 868   score: 1.0   memory length: 156545   epsilon: 0.8880389200024306    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 869   score: 4.0   memory length: 156810   epsilon: 0.887514220002442    steps: 265    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 870   score: 1.0   memory length: 156961   epsilon: 0.8872152400024484    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 871   score: 3.0   memory length: 157187   epsilon: 0.8867677600024582    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 872   score: 0.0   memory length: 157310   epsilon: 0.8865242200024634    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 873   score: 1.0   memory length: 157479   epsilon: 0.8861896000024707    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 874   score: 0.0   memory length: 157601   epsilon: 0.885948040002476    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 875   score: 0.0   memory length: 157724   epsilon: 0.8857045000024812    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 876   score: 2.0   memory length: 157922   epsilon: 0.8853124600024898    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 877   score: 2.0   memory length: 158120   epsilon: 0.8849204200024983    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 878   score: 4.0   memory length: 158436   epsilon: 0.8842947400025118    steps: 316    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 879   score: 2.0   memory length: 158616   epsilon: 0.8839383400025196    steps: 180    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 880   score: 1.0   memory length: 158767   epsilon: 0.8836393600025261    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 881   score: 2.0   memory length: 158965   epsilon: 0.8832473200025346    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 882   score: 0.0   memory length: 159088   epsilon: 0.8830037800025399    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 883   score: 2.0   memory length: 159304   epsilon: 0.8825761000025492    steps: 216    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 884   score: 3.0   memory length: 159550   epsilon: 0.8820890200025597    steps: 246    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 885   score: 0.0   memory length: 159672   epsilon: 0.881847460002565    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 886   score: 2.0   memory length: 159853   epsilon: 0.8814890800025728    steps: 181    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 887   score: 1.0   memory length: 160004   epsilon: 0.8811901000025792    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 888   score: 3.0   memory length: 160271   epsilon: 0.8806614400025907    steps: 267    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 889   score: 2.0   memory length: 160468   epsilon: 0.8802713800025992    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 890   score: 1.0   memory length: 160638   epsilon: 0.8799347800026065    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 891   score: 1.0   memory length: 160789   epsilon: 0.879635800002613    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 892   score: 0.0   memory length: 160912   epsilon: 0.8793922600026183    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 893   score: 2.0   memory length: 161129   epsilon: 0.8789626000026276    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 894   score: 2.0   memory length: 161327   epsilon: 0.8785705600026361    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 895   score: 0.0   memory length: 161449   epsilon: 0.8783290000026414    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 896   score: 0.0   memory length: 161572   epsilon: 0.8780854600026466    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 897   score: 2.0   memory length: 161770   epsilon: 0.8776934200026552    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 898   score: 1.0   memory length: 161920   epsilon: 0.8773964200026616    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 899   score: 1.0   memory length: 162089   epsilon: 0.8770618000026689    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 900   score: 1.0   memory length: 162259   epsilon: 0.8767252000026762    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 901   score: 0.0   memory length: 162382   epsilon: 0.8764816600026815    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 902   score: 1.0   memory length: 162533   epsilon: 0.876182680002688    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 903   score: 5.0   memory length: 162857   epsilon: 0.8755411600027019    steps: 324    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 904   score: 6.0   memory length: 163232   epsilon: 0.874798660002718    steps: 375    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 905   score: 1.0   memory length: 163382   epsilon: 0.8745016600027244    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 906   score: 2.0   memory length: 163582   epsilon: 0.874105660002733    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 907   score: 1.0   memory length: 163750   epsilon: 0.8737730200027403    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 908   score: 1.0   memory length: 163922   epsilon: 0.8734324600027477    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 909   score: 2.0   memory length: 164120   epsilon: 0.8730404200027562    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 910   score: 2.0   memory length: 164338   epsilon: 0.8726087800027655    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 911   score: 0.0   memory length: 164461   epsilon: 0.8723652400027708    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 912   score: 2.0   memory length: 164679   epsilon: 0.8719336000027802    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 913   score: 3.0   memory length: 164906   epsilon: 0.87148414000279    steps: 227    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 914   score: 4.0   memory length: 165184   epsilon: 0.8709337000028019    steps: 278    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 915   score: 3.0   memory length: 165412   epsilon: 0.8704822600028117    steps: 228    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 916   score: 1.0   memory length: 165581   epsilon: 0.870147640002819    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 917   score: 0.0   memory length: 165703   epsilon: 0.8699060800028242    steps: 122    lr: 0.0001     evaluation reward: 1.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 918   score: 2.0   memory length: 165920   epsilon: 0.8694764200028335    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 919   score: 2.0   memory length: 166118   epsilon: 0.869084380002842    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 920   score: 0.0   memory length: 166240   epsilon: 0.8688428200028473    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 921   score: 0.0   memory length: 166363   epsilon: 0.8685992800028526    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 922   score: 4.0   memory length: 166622   epsilon: 0.8680864600028637    steps: 259    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 923   score: 0.0   memory length: 166745   epsilon: 0.867842920002869    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 924   score: 2.0   memory length: 166943   epsilon: 0.8674508800028775    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 925   score: 1.0   memory length: 167112   epsilon: 0.8671162600028848    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 926   score: 6.0   memory length: 167475   epsilon: 0.8663975200029004    steps: 363    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 927   score: 3.0   memory length: 167724   epsilon: 0.8659045000029111    steps: 249    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 928   score: 2.0   memory length: 167922   epsilon: 0.8655124600029196    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 929   score: 0.0   memory length: 168045   epsilon: 0.8652689200029249    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 930   score: 2.0   memory length: 168261   epsilon: 0.8648412400029342    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 931   score: 2.0   memory length: 168478   epsilon: 0.8644115800029435    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 932   score: 2.0   memory length: 168675   epsilon: 0.864021520002952    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 933   score: 0.0   memory length: 168798   epsilon: 0.8637779800029572    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 934   score: 1.0   memory length: 168970   epsilon: 0.8634374200029646    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 935   score: 0.0   memory length: 169093   epsilon: 0.8631938800029699    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 936   score: 0.0   memory length: 169215   epsilon: 0.8629523200029752    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 937   score: 0.0   memory length: 169338   epsilon: 0.8627087800029805    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 938   score: 4.0   memory length: 169631   epsilon: 0.862128640002993    steps: 293    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 939   score: 2.0   memory length: 169829   epsilon: 0.8617366000030016    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 940   score: 1.0   memory length: 169998   epsilon: 0.8614019800030088    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 941   score: 2.0   memory length: 170196   epsilon: 0.8610099400030173    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 942   score: 0.0   memory length: 170318   epsilon: 0.8607683800030226    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 943   score: 0.0   memory length: 170440   epsilon: 0.8605268200030278    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 944   score: 1.0   memory length: 170611   epsilon: 0.8601882400030352    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 945   score: 2.0   memory length: 170829   epsilon: 0.8597566000030445    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 946   score: 3.0   memory length: 171055   epsilon: 0.8593091200030543    steps: 226    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 947   score: 3.0   memory length: 171322   epsilon: 0.8587804600030657    steps: 267    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 948   score: 3.0   memory length: 171567   epsilon: 0.8582953600030763    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 949   score: 0.0   memory length: 171690   epsilon: 0.8580518200030816    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 950   score: 2.0   memory length: 171887   epsilon: 0.85766176000309    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 951   score: 3.0   memory length: 172139   epsilon: 0.8571628000031009    steps: 252    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 952   score: 0.0   memory length: 172262   epsilon: 0.8569192600031061    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 953   score: 2.0   memory length: 172460   epsilon: 0.8565272200031147    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 954   score: 2.0   memory length: 172660   epsilon: 0.8561312200031232    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 955   score: 4.0   memory length: 172955   epsilon: 0.8555471200031359    steps: 295    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 956   score: 0.0   memory length: 173077   epsilon: 0.8553055600031412    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 957   score: 1.0   memory length: 173229   epsilon: 0.8550046000031477    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 958   score: 2.0   memory length: 173428   epsilon: 0.8546105800031563    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 959   score: 3.0   memory length: 173675   epsilon: 0.8541215200031669    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 960   score: 2.0   memory length: 173873   epsilon: 0.8537294800031754    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 961   score: 1.0   memory length: 174024   epsilon: 0.8534305000031819    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 962   score: 3.0   memory length: 174268   epsilon: 0.8529473800031924    steps: 244    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 963   score: 1.0   memory length: 174419   epsilon: 0.8526484000031989    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 964   score: 1.0   memory length: 174571   epsilon: 0.8523474400032054    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 965   score: 3.0   memory length: 174799   epsilon: 0.8518960000032152    steps: 228    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 966   score: 3.0   memory length: 175062   epsilon: 0.8513752600032265    steps: 263    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 967   score: 3.0   memory length: 175331   epsilon: 0.8508426400032381    steps: 269    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 968   score: 2.0   memory length: 175529   epsilon: 0.8504506000032466    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 969   score: 0.0   memory length: 175651   epsilon: 0.8502090400032518    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 970   score: 1.0   memory length: 175820   epsilon: 0.8498744200032591    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 971   score: 4.0   memory length: 176096   epsilon: 0.8493279400032709    steps: 276    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 972   score: 0.0   memory length: 176218   epsilon: 0.8490863800032762    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 973   score: 2.0   memory length: 176418   epsilon: 0.8486903800032848    steps: 200    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 974   score: 4.0   memory length: 176693   epsilon: 0.8481458800032966    steps: 275    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 975   score: 2.0   memory length: 176891   epsilon: 0.8477538400033051    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 976   score: 1.0   memory length: 177042   epsilon: 0.8474548600033116    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 977   score: 4.0   memory length: 177319   epsilon: 0.8469064000033235    steps: 277    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 978   score: 3.0   memory length: 177564   epsilon: 0.846421300003334    steps: 245    lr: 0.0001     evaluation reward: 1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 979   score: 2.0   memory length: 177762   epsilon: 0.8460292600033426    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 980   score: 4.0   memory length: 178016   epsilon: 0.8455263400033535    steps: 254    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 981   score: 2.0   memory length: 178216   epsilon: 0.8451303400033621    steps: 200    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 982   score: 2.0   memory length: 178414   epsilon: 0.8447383000033706    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 983   score: 2.0   memory length: 178630   epsilon: 0.8443106200033799    steps: 216    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 984   score: 3.0   memory length: 178857   epsilon: 0.8438611600033896    steps: 227    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 985   score: 3.0   memory length: 179125   epsilon: 0.8433305200034011    steps: 268    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 986   score: 3.0   memory length: 179352   epsilon: 0.8428810600034109    steps: 227    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 987   score: 2.0   memory length: 179567   epsilon: 0.8424553600034201    steps: 215    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 988   score: 2.0   memory length: 179765   epsilon: 0.8420633200034287    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 989   score: 2.0   memory length: 179963   epsilon: 0.8416712800034372    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 990   score: 1.0   memory length: 180131   epsilon: 0.8413386400034444    steps: 168    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 991   score: 2.0   memory length: 180334   epsilon: 0.8409367000034531    steps: 203    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 992   score: 2.0   memory length: 180532   epsilon: 0.8405446600034616    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 993   score: 1.0   memory length: 180701   epsilon: 0.8402100400034689    steps: 169    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 994   score: 1.0   memory length: 180852   epsilon: 0.8399110600034754    steps: 151    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 995   score: 2.0   memory length: 181050   epsilon: 0.8395190200034839    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 996   score: 1.0   memory length: 181201   epsilon: 0.8392200400034904    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 997   score: 1.0   memory length: 181352   epsilon: 0.8389210600034969    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 998   score: 0.0   memory length: 181475   epsilon: 0.8386775200035022    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 999   score: 4.0   memory length: 181754   epsilon: 0.8381251000035141    steps: 279    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1000   score: 0.0   memory length: 181877   epsilon: 0.8378815600035194    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1001   score: 1.0   memory length: 182028   epsilon: 0.8375825800035259    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1002   score: 2.0   memory length: 182226   epsilon: 0.8371905400035344    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1003   score: 2.0   memory length: 182444   epsilon: 0.8367589000035438    steps: 218    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1004   score: 3.0   memory length: 182695   epsilon: 0.8362619200035546    steps: 251    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1005   score: 2.0   memory length: 182893   epsilon: 0.8358698800035631    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1006   score: 2.0   memory length: 183111   epsilon: 0.8354382400035725    steps: 218    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1007   score: 1.0   memory length: 183282   epsilon: 0.8350996600035798    steps: 171    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1008   score: 4.0   memory length: 183576   epsilon: 0.8345175400035925    steps: 294    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1009   score: 2.0   memory length: 183774   epsilon: 0.834125500003601    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1010   score: 3.0   memory length: 184000   epsilon: 0.8336780200036107    steps: 226    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1011   score: 2.0   memory length: 184198   epsilon: 0.8332859800036192    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1012   score: 2.0   memory length: 184415   epsilon: 0.8328563200036285    steps: 217    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1013   score: 2.0   memory length: 184633   epsilon: 0.8324246800036379    steps: 218    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1014   score: 1.0   memory length: 184783   epsilon: 0.8321276800036443    steps: 150    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1015   score: 2.0   memory length: 184998   epsilon: 0.8317019800036536    steps: 215    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1016   score: 0.0   memory length: 185120   epsilon: 0.8314604200036588    steps: 122    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1017   score: 2.0   memory length: 185318   epsilon: 0.8310683800036673    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1018   score: 2.0   memory length: 185519   epsilon: 0.830670400003676    steps: 201    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1019   score: 1.0   memory length: 185689   epsilon: 0.8303338000036833    steps: 170    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1020   score: 1.0   memory length: 185860   epsilon: 0.8299952200036906    steps: 171    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1021   score: 2.0   memory length: 186058   epsilon: 0.8296031800036991    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1022   score: 1.0   memory length: 186227   epsilon: 0.8292685600037064    steps: 169    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1023   score: 0.0   memory length: 186349   epsilon: 0.8290270000037117    steps: 122    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1024   score: 1.0   memory length: 186499   epsilon: 0.8287300000037181    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1025   score: 0.0   memory length: 186622   epsilon: 0.8284864600037234    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1026   score: 1.0   memory length: 186773   epsilon: 0.8281874800037299    steps: 151    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1027   score: 3.0   memory length: 186998   epsilon: 0.8277419800037396    steps: 225    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1028   score: 3.0   memory length: 187227   epsilon: 0.8272885600037494    steps: 229    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1029   score: 1.0   memory length: 187396   epsilon: 0.8269539400037567    steps: 169    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1030   score: 1.0   memory length: 187564   epsilon: 0.8266213000037639    steps: 168    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1031   score: 0.0   memory length: 187687   epsilon: 0.8263777600037692    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1032   score: 3.0   memory length: 187917   epsilon: 0.825922360003779    steps: 230    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1033   score: 3.0   memory length: 188127   epsilon: 0.8255065600037881    steps: 210    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1034   score: 1.0   memory length: 188278   epsilon: 0.8252075800037946    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1035   score: 2.0   memory length: 188459   epsilon: 0.8248492000038024    steps: 181    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1036   score: 3.0   memory length: 188685   epsilon: 0.8244017200038121    steps: 226    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1037   score: 1.0   memory length: 188854   epsilon: 0.8240671000038193    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1038   score: 1.0   memory length: 189005   epsilon: 0.8237681200038258    steps: 151    lr: 0.0001     evaluation reward: 1.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1039   score: 4.0   memory length: 189263   epsilon: 0.8232572800038369    steps: 258    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1040   score: 3.0   memory length: 189510   epsilon: 0.8227682200038475    steps: 247    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1041   score: 5.0   memory length: 189808   epsilon: 0.8221781800038603    steps: 298    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1042   score: 1.0   memory length: 189976   epsilon: 0.8218455400038676    steps: 168    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1043   score: 0.0   memory length: 190099   epsilon: 0.8216020000038728    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1044   score: 3.0   memory length: 190346   epsilon: 0.8211129400038835    steps: 247    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1045   score: 1.0   memory length: 190497   epsilon: 0.82081396000389    steps: 151    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1046   score: 3.0   memory length: 190743   epsilon: 0.8203268800039005    steps: 246    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1047   score: 2.0   memory length: 190941   epsilon: 0.819934840003909    steps: 198    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1048   score: 1.0   memory length: 191091   epsilon: 0.8196378400039155    steps: 150    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1049   score: 2.0   memory length: 191288   epsilon: 0.819247780003924    steps: 197    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1050   score: 2.0   memory length: 191488   epsilon: 0.8188517800039326    steps: 200    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1051   score: 3.0   memory length: 191716   epsilon: 0.8184003400039424    steps: 228    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1052   score: 2.0   memory length: 191914   epsilon: 0.8180083000039509    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1053   score: 3.0   memory length: 192144   epsilon: 0.8175529000039607    steps: 230    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1054   score: 4.0   memory length: 192418   epsilon: 0.8170103800039725    steps: 274    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 1055   score: 0.0   memory length: 192541   epsilon: 0.8167668400039778    steps: 123    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1056   score: 2.0   memory length: 192739   epsilon: 0.8163748000039863    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1057   score: 0.0   memory length: 192862   epsilon: 0.8161312600039916    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1058   score: 2.0   memory length: 193079   epsilon: 0.8157016000040009    steps: 217    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1059   score: 1.0   memory length: 193250   epsilon: 0.8153630200040083    steps: 171    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1060   score: 1.0   memory length: 193421   epsilon: 0.8150244400040156    steps: 171    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1061   score: 2.0   memory length: 193636   epsilon: 0.8145987400040249    steps: 215    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1062   score: 0.0   memory length: 193758   epsilon: 0.8143571800040301    steps: 122    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1063   score: 2.0   memory length: 193955   epsilon: 0.8139671200040386    steps: 197    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1064   score: 2.0   memory length: 194171   epsilon: 0.8135394400040479    steps: 216    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1065   score: 0.0   memory length: 194294   epsilon: 0.8132959000040532    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1066   score: 5.0   memory length: 194604   epsilon: 0.8126821000040665    steps: 310    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1067   score: 0.0   memory length: 194727   epsilon: 0.8124385600040718    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1068   score: 3.0   memory length: 194975   epsilon: 0.8119475200040824    steps: 248    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1069   score: 0.0   memory length: 195097   epsilon: 0.8117059600040877    steps: 122    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1070   score: 2.0   memory length: 195295   epsilon: 0.8113139200040962    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1071   score: 3.0   memory length: 195539   epsilon: 0.8108308000041067    steps: 244    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1072   score: 0.0   memory length: 195661   epsilon: 0.8105892400041119    steps: 122    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1073   score: 0.0   memory length: 195783   epsilon: 0.8103476800041172    steps: 122    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1074   score: 0.0   memory length: 195905   epsilon: 0.8101061200041224    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1075   score: 3.0   memory length: 196130   epsilon: 0.8096606200041321    steps: 225    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1076   score: 4.0   memory length: 196385   epsilon: 0.809155720004143    steps: 255    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1077   score: 4.0   memory length: 196673   epsilon: 0.8085854800041554    steps: 288    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1078   score: 4.0   memory length: 196954   epsilon: 0.8080291000041675    steps: 281    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1079   score: 3.0   memory length: 197183   epsilon: 0.8075756800041773    steps: 229    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1080   score: 2.0   memory length: 197381   epsilon: 0.8071836400041859    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1081   score: 1.0   memory length: 197549   epsilon: 0.8068510000041931    steps: 168    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1082   score: 2.0   memory length: 197768   epsilon: 0.8064173800042025    steps: 219    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1083   score: 2.0   memory length: 197986   epsilon: 0.8059857400042119    steps: 218    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1084   score: 4.0   memory length: 198282   epsilon: 0.8053996600042246    steps: 296    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1085   score: 3.0   memory length: 198547   epsilon: 0.804874960004236    steps: 265    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1086   score: 1.0   memory length: 198716   epsilon: 0.8045403400042432    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1087   score: 1.0   memory length: 198885   epsilon: 0.8042057200042505    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1088   score: 1.0   memory length: 199054   epsilon: 0.8038711000042578    steps: 169    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1089   score: 4.0   memory length: 199329   epsilon: 0.8033266000042696    steps: 275    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1090   score: 3.0   memory length: 199578   epsilon: 0.8028335800042803    steps: 249    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1091   score: 3.0   memory length: 199804   epsilon: 0.80238610000429    steps: 226    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1092   score: 5.0   memory length: 200110   epsilon: 0.8017802200043032    steps: 306    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1093   score: 0.0   memory length: 200233   epsilon: 0.8015366800043084    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1094   score: 3.0   memory length: 200480   epsilon: 0.8010476200043191    steps: 247    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1095   score: 0.0   memory length: 200603   epsilon: 0.8008040800043243    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1096   score: 1.0   memory length: 200772   epsilon: 0.8004694600043316    steps: 169    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1097   score: 2.0   memory length: 200974   epsilon: 0.8000695000043403    steps: 202    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1098   score: 3.0   memory length: 201244   epsilon: 0.7995349000043519    steps: 270    lr: 0.0001     evaluation reward: 1.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1099   score: 0.0   memory length: 201366   epsilon: 0.7992933400043571    steps: 122    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1100   score: 0.0   memory length: 201488   epsilon: 0.7990517800043624    steps: 122    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1101   score: 0.0   memory length: 201611   epsilon: 0.7988082400043677    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1102   score: 3.0   memory length: 201858   epsilon: 0.7983191800043783    steps: 247    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1103   score: 1.0   memory length: 202009   epsilon: 0.7980202000043848    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1104   score: 3.0   memory length: 202256   epsilon: 0.7975311400043954    steps: 247    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1105   score: 1.0   memory length: 202407   epsilon: 0.7972321600044019    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1106   score: 1.0   memory length: 202558   epsilon: 0.7969331800044084    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1107   score: 2.0   memory length: 202756   epsilon: 0.7965411400044169    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1108   score: 2.0   memory length: 202954   epsilon: 0.7961491000044254    steps: 198    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1109   score: 3.0   memory length: 203180   epsilon: 0.7957016200044351    steps: 226    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1110   score: 3.0   memory length: 203428   epsilon: 0.7952105800044458    steps: 248    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1111   score: 0.0   memory length: 203551   epsilon: 0.7949670400044511    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1112   score: 5.0   memory length: 203877   epsilon: 0.7943215600044651    steps: 326    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1113   score: 4.0   memory length: 204155   epsilon: 0.793771120004477    steps: 278    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1114   score: 1.0   memory length: 204305   epsilon: 0.7934741200044835    steps: 150    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1115   score: 1.0   memory length: 204474   epsilon: 0.7931395000044907    steps: 169    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1116   score: 0.0   memory length: 204597   epsilon: 0.792895960004496    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1117   score: 1.0   memory length: 204747   epsilon: 0.7925989600045025    steps: 150    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1118   score: 2.0   memory length: 204944   epsilon: 0.7922089000045109    steps: 197    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1119   score: 9.0   memory length: 205333   epsilon: 0.7914386800045277    steps: 389    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1120   score: 2.0   memory length: 205551   epsilon: 0.791007040004537    steps: 218    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 1121   score: 4.0   memory length: 205844   epsilon: 0.7904269000045496    steps: 293    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1122   score: 2.0   memory length: 206065   epsilon: 0.7899893200045591    steps: 221    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1123   score: 2.0   memory length: 206283   epsilon: 0.7895576800045685    steps: 218    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1124   score: 2.0   memory length: 206481   epsilon: 0.789165640004577    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1125   score: 4.0   memory length: 206778   epsilon: 0.7885775800045898    steps: 297    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1126   score: 0.0   memory length: 206900   epsilon: 0.788336020004595    steps: 122    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1127   score: 3.0   memory length: 207145   epsilon: 0.7878509200046055    steps: 245    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1128   score: 3.0   memory length: 207392   epsilon: 0.7873618600046162    steps: 247    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1129   score: 4.0   memory length: 207667   epsilon: 0.786817360004628    steps: 275    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1130   score: 1.0   memory length: 207818   epsilon: 0.7865183800046345    steps: 151    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1131   score: 0.0   memory length: 207941   epsilon: 0.7862748400046398    steps: 123    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1132   score: 2.0   memory length: 208159   epsilon: 0.7858432000046491    steps: 218    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1133   score: 2.0   memory length: 208377   epsilon: 0.7854115600046585    steps: 218    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1134   score: 1.0   memory length: 208546   epsilon: 0.7850769400046658    steps: 169    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1135   score: 1.0   memory length: 208697   epsilon: 0.7847779600046723    steps: 151    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1136   score: 6.0   memory length: 209090   epsilon: 0.7839998200046892    steps: 393    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1137   score: 0.0   memory length: 209213   epsilon: 0.7837562800046944    steps: 123    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1138   score: 2.0   memory length: 209429   epsilon: 0.7833286000047037    steps: 216    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1139   score: 2.0   memory length: 209627   epsilon: 0.7829365600047122    steps: 198    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1140   score: 4.0   memory length: 209941   epsilon: 0.7823148400047257    steps: 314    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1141   score: 2.0   memory length: 210158   epsilon: 0.7818851800047351    steps: 217    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1142   score: 3.0   memory length: 210368   epsilon: 0.7814693800047441    steps: 210    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1143   score: 0.0   memory length: 210490   epsilon: 0.7812278200047493    steps: 122    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1144   score: 2.0   memory length: 210690   epsilon: 0.7808318200047579    steps: 200    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1145   score: 3.0   memory length: 210937   epsilon: 0.7803427600047685    steps: 247    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1146   score: 0.0   memory length: 211060   epsilon: 0.7800992200047738    steps: 123    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1147   score: 0.0   memory length: 211183   epsilon: 0.7798556800047791    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1148   score: 3.0   memory length: 211430   epsilon: 0.7793666200047897    steps: 247    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1149   score: 2.0   memory length: 211648   epsilon: 0.7789349800047991    steps: 218    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1150   score: 2.0   memory length: 211846   epsilon: 0.7785429400048076    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1151   score: 2.0   memory length: 212062   epsilon: 0.7781152600048169    steps: 216    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1152   score: 6.0   memory length: 212451   epsilon: 0.7773450400048336    steps: 389    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1153   score: 2.0   memory length: 212667   epsilon: 0.7769173600048429    steps: 216    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1154   score: 2.0   memory length: 212883   epsilon: 0.7764896800048522    steps: 216    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1155   score: 2.0   memory length: 213081   epsilon: 0.7760976400048607    steps: 198    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1156   score: 0.0   memory length: 213204   epsilon: 0.775854100004866    steps: 123    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1157   score: 6.0   memory length: 213525   epsilon: 0.7752185200048798    steps: 321    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1158   score: 2.0   memory length: 213724   epsilon: 0.7748245000048883    steps: 199    lr: 0.0001     evaluation reward: 2.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1159   score: 2.0   memory length: 213943   epsilon: 0.7743908800048978    steps: 219    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1160   score: 2.0   memory length: 214140   epsilon: 0.7740008200049062    steps: 197    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1161   score: 0.0   memory length: 214262   epsilon: 0.7737592600049115    steps: 122    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1162   score: 0.0   memory length: 214385   epsilon: 0.7735157200049168    steps: 123    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1163   score: 1.0   memory length: 214554   epsilon: 0.773181100004924    steps: 169    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1164   score: 2.0   memory length: 214772   epsilon: 0.7727494600049334    steps: 218    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1165   score: 4.0   memory length: 215013   epsilon: 0.7722722800049437    steps: 241    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 1166   score: 1.0   memory length: 215164   epsilon: 0.7719733000049502    steps: 151    lr: 0.0001     evaluation reward: 2.07\n"
     ]
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
